%!TEX root = ../Dissertation.texIn this section we will show an important family of functions $g$ and $H$, theproximal operator~\eqref{eq:2} can be computed efficiently via an interiormethod. This approach builds on the work of \cite{aravkin2013sparse}, whodefine the class of quadratic- support functions and outline a particularinterior algorithm for their optimization.  Our approach is specialized to thecase where the quadratic- support function appears inside of a proximalcomputation.  Together with the correct dualization approach (\S\ref{sec:prox-to-qp}), this yields a particularly efficient interior implementation when thedata that define $g$ and $H$ have special structure (\S\ref{sec:eval-prox-oper}). The proximal quasi-Newton method serves as a showcase for how thistechnique can be used within a broader algorithmic context (\S\ref{sec:quasi-Newton}).\section{Quadratic-support functions} \label{sec:problem-setup}\cite{aravkin2013sparse} introduce the notion of aquadratic-support (QS) function, which is a generalization ofsublinear support functions~\cite[Ch.~8E]{RTRW:1998}. Here weintroduce a slightly more general definition than the versionimplemented by Aravkin et al.\@ We retain the ``QS'' designationbecause the quadratic term, which is an essential feature of theirdefinition, can also be expressed by the version we use here.Let $\mathbf{B}_p=\{z\mid \|z\|_p\le1\}$ and$\mathcal{K} = \mathcal{K}_1 \times \cdots \times \mathcal{K}_k$, where each cone$\mathcal{K}_i$ is either a nonnegative orthant $\Real_+^m$ or asecond-order cone$\mathcal{Q}^m=\{(\tau,z)\in\Real\times\Real^{m-1} | z\in\tau\mathbf{B}_2\}$.(The size $m$ of the cones may of course be different for each index$i$.)  The notation $Ay\succeq_\mathcal{K} b$ means that $Ay-b\in\mathcal{K}$, and$\tau\mathbf{B}_p\equiv\{\tau z | z \in\mathbf{B}_p\}$. The indicator on aconvex set $U$ is denoted as\[\delta(x\mid U) = \begin{cases}                   0        & \text{if $x\in U$,}                 \\ +\infty & \text{otherwise.}                  \end{cases}\]Unless otherwise specified, $x$ is an $n$-vector.  To helpdisambiguate dimensions, the $p$-by-$p$ identity matrix is denoted by$I_p$, and the $p$-vector of all ones by $\mathbf{1}_p$.We consider the class of functions$g:\Real^n\to\Real\cup\{+\infty\}$ that have the conjugaterepresentation\begin{equation}  \label{eq:qs}  g(x) = \sup_y\{y^T(Bx+d) \mid y\in\mathcal{Y}\},\quad   \text{where}\quad   \mathcal{Y} = \{y\in \Real^\ell \mid Ay \succeq_\mathcal{K} b\}.\end{equation}(The term ``conjugate'' alludes to the implicit duality, since $g$ maybe considered as the conjugate of the indicator function to the set$\mathcal{Y}$.)  We assume throughout that the feasible set$\mathcal{Y}$ is nonempty. If $\mathcal{Y}$ containsthe origin, the QS function $g$ is nonnegative for all $x$, and we canthen consider it to be a penalty function. This is automatically true,for example, if $b\le0$.The formulation~\eqref{eq:qs} is close to the standard definition of asublinear support function \cite[\S13]{Roc70}, which is recovered bysetting $d=0$ and $B=I$, and letting $\mathcal{Y}$ be any convex set. Unlikea standard support function, $g$ is not positively homogeneous if$d\ne0$. This is a feature that allows us to capture important classesof penalty functions that are not positively homogeneous, such aspiecewise quadratic functions, the ``deadzone'' penalty, or indicatorson certain constraint sets. These are examples that are notrepresentable by the standard definition of a support function.  Ourdefinition springs from the quadratic-support function definitionintroduced by \cite{aravkin2013sparse}, who additionally allow for anexplicit quadratic term in the objective and for $\mathcal{Y}$ to be anynonempty convex set. The concrete implementation considered by\cite{aravkin2013sparse}, however, is restricted to the casewhere $\mathcal{Y}$ is polyhedral. In contrast, we also allow $\mathcal{K}$ tocontain second-order cones. Therefore, any quadratic objective termsin the \cite{aravkin2013sparse}\ definition can be ``lifted''and turned into a linear term with a second-order-cone constraint (seeExample~\ref{ex:2-norm}). Our definition is thus no less general.This expressive class of functions includes many penalty functionscommonly used in machine learning; \cite{aravkin2013sparse}\give many other examples. In addition, they show how to interpret QSfunctions as the negative log of an associated probability density,which makes these functions relevant to maximum a posterioriestimation.  In the remainder of this section we provide some examplesthat illustrate various regularizing functions and constraints thatcan be expressed as QS functions.\begin{example}[1-norm regularizer] \label{L1_Example}  The 1-norm has the QS representation  \[    \|x\|_1 = \sup_y\{\, y^T x \mid y\in\mathbf{B}_\infty\,\},  \]  where\begin{equation} \label{eq:l1-rep}      A = \pmat{\phantom-I_n\\-I_n},\quad b = -{\pmat{\mathbf{1}_n\\\mathbf{1}_n}},\quad d= 0, \quad B=I_n,\quad \mathcal{K} = \Real_+^{2n},\end{equation}\end{example}\begin{example}[2-norm] \label{ex:2-norm} This simple example  illustrates how the QS representation~\eqref{eq:qs} can represent  the 2-norm, which is not possible using the QS formulation described  by \cite{aravkin2013sparse} where the constraints are  polyhedral. With our definition, the 2-norm has the QS  representation\[\|x\|_2 =\sup_y\{y^T x \mid y\in\mathbf{B}_2\} =\sup_y\{ y^T x \mid (1,y)\succeq_{\mathcal{K}}0\},\]where\begin{equation}\label{eq:1}        A = \pmat{0\\ I_n},  \quad b = \pmat{1\\ 0},  \quad d = 0,  \quad B = I_n,  \quad \mathcal{K} = \mathcal{Q}^{n+1}.\end{equation}\end{example}\begin{example}[Polyhedral norms] \label{ex:norms} Any  polyhedral seminorm is a support function, e.g., $\|Bx\|_1$ for  some matrix $B$. In particular, if the set $\{y|Ay\ge b\}$  contains the origin and is centro-symmetric, then  \[    \|x\| := \sup_y \{ y^T Bx \mid Ay\ge b\}  \]  defines a norm if $B$ is nonsingular, and a seminorm otherwise.  This is a QS function with $d:=0$ (as will be the case for any  positively homogeneous QS function) and $\mathcal{Y}:=\{y \mid Ay\ge b\}$.\end{example}\begin{example}[Quadratic function]\label{ex:quadratic-function}  This example justifies the term ``quadratic'' in our modified  definition, even though there are no explicit quadratic terms. It  also illustrates the roles of the terms $B$ and $d$.  The quadratic  function can be written as  \[  \tfrac{1}{2}\|x\|_2^2  = \sup_{y,\,t}\{ y^T x - \tfrac{1}{2} t \mid \|y\|^2_2\le t \}  = \sup_{y,\,t}    \left\{\! \left.      \pmat{y\\t}^T\left[\pmat{I_n\\0}x-\pmat{0\\\tfrac{1}{2}}\right] \right|      \|y\|_2^2\le t    \!\right\}.  \]  Use the derivation in \ref{sec:qs-soc} to obtain the QS  representation with parameters  \[          A = \pmat{0&\tfrac{1}{2}\\0&\tfrac12\\I_n&0},    \quad b = \pmat{{\phantom-\tfrac12}\\-\tfrac12\\\phantom-0},    \quad d = \pmat{0\\\tfrac12},    \quad B = \pmat{I_n\\0},    \quad \mathcal{K} = \mathcal{Q}^{n+2}.\]\end{example}\begin{example}[1-norm indicator]  \label{ex:1-norm-ball}  This example is closely related to the 1-norm regularizer in  Example~\ref{L1_Example}, except that the QS function is used to  express the constraint $\|{x}\|_1\le 1$ via an indicator function  $g=\delta(\,\cdot\mid\mathbf{B}_1)$.  Write the indicator to the 1-norm  ball as the conjugate of the infinity norm, which gives  \begin{equation}    \label{eq:1-norm-indicator}\begin{aligned}  \delta(x\mid \mathbf{B}_1)     &= \sup_{y}\{y^T x - \|y\|_\infty\}= \sup_{y,\,\tau}\{y^T x-\tau \mid y\in\tau\mathbf{B}_\infty \}      = \sup_{y,\,\tau}\{y^T x-\tau \mid -\tau\mathbf{1}_n \leq y\leq \tau\mathbf{1}_n \}.\end{aligned}\end{equation}This is a QS function with parameters\[        A = \pmat{-I_n&\mathbf{1}_n\\ \phantom-I_n & \mathbf{1}_n},\quad b = 0,\quad d = \pmat{\phantom-0\\-1},\quad B = \pmat{I_n\\0},\quad \mathcal{K} = \Real^{2n}.\]\end{example}\begin{example}[Indicators on polyhedral cones]Consider the following polyhedral cone and its polar:\[  U=\{x \mid Bx\le0\} \quad \text{and} \quad U^\circ=\{B^T y\mid|y\le 0\}.\]Use the support-function representation of a cone in terms of itspolar to obtain\begin{equation}\label{eq:Indicator_Polyhedral_Cone}\delta(x\mid U) = \delta^*(\cdot\mid U^\circ)(x)                = \sup_y \{y^T B x \mid y\leq 0 \},\end{equation}which is an example of an elementary QS function. (See\cite{RTRW:1998} for definitions of the polar of a convex set, andthe convex conjugate.) A concrete example is the positive orthant,obtained by choosing $B = I_n$. An important example, used in isotonicregression~\cite{Best1990}, is the monotonic cone$$  U:=\{x \mid x_i\ge x_j,\ \forall (i,j)\in \mathbf{E}\},$$Here, $ \mathbf{E}$ is the set of edges in a graph $\mathbf{G}=(\mathbf{V}, \mathbf{E})$that describes the relationships between variables in $\mathbf{V}$. If weset $B$ to be the incidence matrix for the graph, \eqref{eq:Indicator_Polyhedral_Cone} thencorresponds to the indicator on the monotonic cone $U$.\end{example}\begin{example}[Distance to a cone] The distance to a cone $U$ that is  a combination of polyhedral and second-order cones can be  represented as a QS function:  \begin{align*}  \inf_{x\in U}\|x-y\|_2  &= \inf_{x}\ \{\|{x-y}\|_2+\delta(x\mid U)\} \\ &= \big[ \delta(\cdot\mid \mathbf{B}_2)+\delta(\cdot\mid U^{\circ})\big]^{*}(y)  =\ \sup \left\{ y^T x \mid y\in \mathbf{B}_2\cap U^{\circ}\right\}.\end{align*}The second equality follows from the relationship between infimalconvolution and conjugates \cite[\S16.4]{Roc70}.  When $U$ is thepositive orthant, for example, $g(x) = \|\max\{0,\,x\}\|_2,$ where the\emph{max} operator is taken elementwise.\end{example}\section{Building quadratic-support functions}\label{sec:qs-calculus}Quadratic-support functions are closed under addition, compositionwith an affine map, and infimal convolution with a quadraticfunction. In the following, let $g_i$ be QS functions with parameters$A_{i}$, $b_{i}$, $d_{i}$, $B_{i}$, and $\mathcal{K}_i$ (with$i=0,1,2$). The rules for addition and composition are described in\cite{aravkin2013sparse}, which are here summarized and amplified.\begin{description}\item[\bf Addition rule]{The function \[h(x):=g_1(x) + g_2(x)\] is QS with parameters\begin{equation*}       A = \pmat{A_1 & \\ & A_2},\quad  b = \pmat{b_{1}\\b_{2}},\quad  d = \pmat{d_{1}\\d_{2}},\quad  B = \pmat{B_{1}\\B_{2}},\quad  \mathcal{K} = \mathcal{K}_1\times \mathcal{K}_2.\end{equation*}}\item[\bf Concatenation rule]{The function\[h(x) := g_0(x^1) + \cdots + g_0(x^k),\]where each partition $x^i\in\Real^n$, is QS with parameters\vspace{-1.5mm}\begin{equation}  \label{Calculus-Rule-Concat}        (A,\,B) = I_k \otimes (A_0,\,B_0),\qquad  (b,\,d) = \mathbf{1}_k \otimes (b_0,\,d_0),\qquad  \mathcal{K} = \mathcal{K}_0 \times \overset{(k)}{\cdots} \times \mathcal{K}_0.\end{equation}where the symbol $\otimes$ denotes the Kronecker product. The rule forconcatenation follows from the rule for addition of QS functions.}\item[\bf Affine composition rule]{The function \[h(x) := g_0(Px-p)\] is QS with parameters\begin{equation*}       A = A_0,\quad  b = b_0,\quad  d = d_0 - B_0p,\quad  B = B_0P.\end{equation*}}\item[Moreau-Yosida regularization]{The Moreau-Yosida envelope of$g_0$ is the value of the proximal operator, i.e.,\begin{equation}\label{eq:4}\mathbf{env}^{H}_{g_0}(z):=\inf_{x}\{\tfrac{1}{2}\|z-x\|_H^{2}+g_0(x)\}.\end{equation}It follows from \cite[Proposition~4.10]{BurkeHoheisel:2013} that\[ \mathbf{env}{H}{g_0}(z) = \sup_y\left\{y^T(B_0x+d_0)-\frac{1}{2} y^T B_0H^{-1} B_0^T y \mid A_0y\succeq_{\mathcal{K}_0} b_0\right\},\]which is a QS function with parameters\begin{equation*}       A = \pmat{0&\tfrac{1}{2}\\0&\tfrac12\\R&0\\A_0&0},\quad  b = \pmat{\phantom-\tfrac12\\-\tfrac12\\0\\b_0},\quad  d = \pmat{d_0\\-\tfrac12},\quad  B = \pmat{B_0\\0},\quad  \mathcal{K} = \mathcal{Q}^{n+2}\times \mathcal{K}_0,\end{equation*}with Cholesky factorization $R^T R = B_0H^{-1} B_0^T$. The derivationis given in \ref{sec:qs-soc}, where we take $Q=B_0H^{-1} B_0^T$.}\end{description}\begin{example}[Sums of norms] \label{ex:sum-of-norms} In applications  of group sparsity \cite{YuL06,jenatton2010proximal}, various norms  are applied to all partitions of $x=(x^1,\ldots,x^p)$, which  possibly overlap. This produces the QS function  \begin{equation} \label{eq:sum-of-norms-g}    g(x) = \|x^1\| + \cdots + \|x^p\|,  \end{equation}  where each norm in the sum may be different. In particular, consider  the case of adding two norms  $g(x)=\|{x^1\|}_\triangledown + \|{x^2}\|_\vartriangle$. (The  extension to adding three or more norms follows trivially.) First,  we introduce matrices $P_i$ that restrict $x$ to partition $i$,  i.e., $x^i=P_ix$, for $i=1,2$. Then  \[    g(x) = \|P_1 x\|_\triangledown + \|P_2 x\|_\vartriangle.  \]  Then we apply the affine-composition and addition rules to  determine the corresponding quantities that define the QS  representation of $g$:  \[          A = \pmat{A_\triangledown \\& A_\vartriangle},    \quad b = \pmat{b_\triangledown\\b_\vartriangle},    \quad d = 0,    \quad B = \pmat{B_\triangledown P_1\\B_\vartriangle P_2},    \quad \mathcal{K} = \mathcal{K}_\triangledown \times \mathcal{K}_\vartriangle,  \]  where $A_i$, $b_i$, $B_i$, and $\mathcal{K}_i$ (with  $i=\triangledown,\vartriangle$) are the quantities that define the  QS representation of the individual norms. (Necessarily, $d=0$  because the result is a norm and therefore positive homogeneous.) In  the special case where $\|\cdot\|_\triangledown$ and  $\|{\cdot}\|_\vartriangle$ are both the 2-norm, then $A_i$, $b_i$,  $B_i$, and $\mathcal{K}_i$ are given by~\eqref{eq:1} in  Example~\ref{ex:2-norm}.\end{example}\begin{example}[Graph-based 1-norm and total variation]  \label{ex:graph1-norm-example}  A variation of Example~\ref{ex:sum-of-norms} can be used  to define a variety of interesting norms, including the graph-based  1-norm regularizer used in machine learning \cite{chin2013runtime},  and the isotropic and anisotropic versions of total variation (TV),  important in image processing \cite{LouZengOsherYin:2014}. Let  \begin{equation*}    g(x)=\|Nx\|_{G}    \quad\text{with}\quad \|z\|_G = \sum_{i=1}^p \|z^i\|_2,  \end{equation*}  where $z^i$ is a partition of $z$ and $N$ is an $m$-by-$n$  matrix. For anisotropic TV and the graph-based 1-norm regularizer,  $N$ is the adjacency matrix of a graph, and each partition $z^i$ has  a single unique element, so $g(x)=\|{Nx}\|_1$.  For isotropic TV,  each partition captures neighboring pairs of variables, and $N$ is a  finite-difference matrix.  The QS addition and affine-composition  rules can be combined to derive the parameters of $g$. When $p=m$  (i.e., each $z^i$ is a scalar), we are summing $n$ absolute-value  functions, and we use \eqref{eq:l1-rep} and  \eqref{Calculus-Rule-Concat} to obtain  \begin{equation} \label{eq:qs-rep-graph-1-norm}        A = I_m \otimes \pmat{\phantom-1\\-1},  \quad b = \mathbf{1}_m \otimes \pmat{-1\\-1},  \quad d = 0,  \quad B = N,  \quad \mathcal{K} = \Real^{2m}_+.  \end{equation}  Now consider the variation where $p=m/2$, (i.e., each partition has  size 2), which corresponds to summing $m/2$ two-dimensional  2-norms. Use \eqref{eq:1} to obtain\[        A = I_{m/2} \otimes \pmat{0\\ I_2},  \quad b = \mathbf{1}_{m/2} \otimes \pmat{1\\ 0},  \quad d = 0,  \quad B = N,  \quad \mathcal{K} = \mathcal{Q}^2 \times \overset{(m/2)}{\cdots} \times \mathcal{Q}^2.\]\end{example}\section{The proximal operator as a conic QP} \label{sec:prox-to-qp}We describe in this section how the proximal map~\eqref{eq:prox-map}can be obtained as the solution of a quadratic optimization problem(QP) over conic constraints,\begin{align} \label{eq:qp}  \text{minimize}_{y}\quad\tfrac{1}{2} y^T Qy-c^T y \quad \text{such that} \quad Ay\succeq_\mathcal{K} b,\end{align}for some positive semidefinite $\ell$-by-$\ell$ matrix $Q$ and aconvex cone $\mathcal{K}=\mathcal{K}_1\times\cdots\times\mathcal{K}_k$.  Thetransformation to a conic QP is not immediate because thedefinition of the QS function implies that the proximal map involvesnested optimization. Duality, however, furnishes a means forsimplifying this problem.\begin{prop} \label{prop:prox-to-qp} Let $g$ be a QS function. The following problems are dual pairs:\begin{subequations} \label{eq:dual-probs}\begin{alignat}{2}  &\text{\normalfont minimize}_{x} &\quad  &\tfrac{1}{2}\|{z-x}\|^2_H+g(x),  \label{eq:primal-prob}\\  &\text{\normalfont minimize}_{Ay\succeq_\mathcal{K} b} &\quad &\tfrac{1}{2} y^T BH^{-1}B^{T} y-(d+Bz)^T y.  \label{eq:dual-prob}\end{alignat}\end{subequations}If strong duality holds, the corresponding primal-dual solutions arerelated by\begin{equation} \label{eq:3}  Hx+B^T y=Hz.\end{equation}\end{prop}\begin{proof}Let\[  h_1(x) := \tfrac{1}{2}\|x-z\|_{H}^{2} \quad   \text{and} \quad   h_2(x) : =\sup_{y\in\mathcal{Y}} \{ y^T(x+d) \}.\]If strong duality holds, it follows from\cite[Prop.~5.3.8]{bertsekas2009convex} that\begin{equation} \label{eq:strong-duality}  \inf_{x} \{  h_1(x) + h_2(Bx) \}  =- \inf_{y}\{ h_1^*(-B^T y) + h_2^*(y) \},\end{equation}where\[  h_1^{*}(y)  = \tfrac{1}{2}\|y\|_{H^{-1}}^{2}+z^{T}y  \quad \text{and} \quad   h_2^{*}(y)  = \delta(z \mid \mathcal{Y})-d^{T}y\]are the Fenchel conjugates of $h_1$ and $h_2$, and the infima onboth sides are attained. (See \cite[\S12]{Roc70} for the convexcalculus of Fenchel conjugates.)  The right-hand sideof~\eqref{eq:strong-duality} is precisely the dualproblem~\eqref{eq:dual-prob}. It also follows from Fenchel duality that the pair $(x,y)$is optimal only if\[  x \in \text{argmin}_x\{ h_1(x) + y^T B x \}.\]Differentiate this objective to obtain~\eqref{eq:3}.\end{proof}Strong duality holds when$B\cdot\text{ri dom}(h_{1})\cap\text{ri dom}(h_{2})\neq\emptyset.$This holds, for example, when the interior of the domain of $g$ is nonempty, since\[  \text{int dom}(g)\neq\emptyset  \iff  \text{im}\,B\cap\text{int dom}(h_{2})\neq\emptyset  \Rightarrow  B\cdot\text{ri dom}(h_{1})\cap\text{ri dom}(h_{2})\neq\emptyset.\]In all of the examples in this paper, this condition holds true.\section{Primal-dual methods for conic QP} \label{sec:eval-prox-oper}Proposition~\ref{prop:prox-to-qp} provides a means of evaluating theproximal map of QS functions via conic quadratic optimization. Thereare many algorithms for solving convex conic QPs, but primal-dualmethods offer a particularly efficient approach that can leverage thespecial structure that defines the class of QS functions.  A detaileddiscussion of the implementation of primal-dual methods for conicoptimization is given by \cite{vandenberghe:2010}. Here we summarizethe main aspects that pertain to implementing these methodsefficiently in our context.The standard development of primal-dual methods for~\eqref{eq:qp} isbased on perturbing the optimality conditions, which can be stated asfollows. The solution $y$, together with slack and dual vectors $s$and $v$, must satisfy\[  Qy-A^{T}v = c,\quad v \succeq_\mathcal{K} 0, \quad Sv = 0,\]where the matrix $S$ is block diagonal, and each $m_i$-by-$m_i$ block$S_i$ is either a diagonal or arrow matrix depending on the type ofcone, i.e.,\[S_i = \begin{cases}        \mathbf{diag}(s_i)  & \mbox{if $\mathcal{K}_i=\Real^{m_i}_+$,}      \\\mathbf{arrow}(s_i) & \mbox{if $\mathcal{K}_{i}=\mathcal{Q}^{m_i}$,}      \end{cases}\qquad\mathbf{arrow}(u) := \pmat{u_0 & \bar u^T \\ \bar u & u_0 I}\quad\text{for}\quad u = (u_0, \bar u).\]See \cite{vandenberghe:2010} for further details.Now replace the complementarity condition $Sv=0$ with its perturbation$Sv=\mu e $, where $\mu$ is a positive parameter and$e=(e^1,\ldots,e^k)$, with each partition defined by\[e^i =\begin{cases}  (1,1,\ldots,1) & \mbox{if $\mathcal{K}_{i}=\Real_+^{m_i}$,}\\(1,0,\ldots,0) & \mbox{if $\mathcal{K}_{i}=Q^{m_i}$.}\end{cases}\]A Newton-like method is applied to the perturbed optimalityconditions, which we phrase as the root of the function\begin{equation}\label{eq:int_residual}R_{\mu}:\pmat{y\\v\\s}\mapsto\pmat{r_d\\r_p\\r_\mu} :=\pmat{  Qy -A^T v -c\\Ay-s-b\\Sv-\mu e}.\end{equation}Each iteration of the method proceeds by systematically choosing theperturbation parameter $\mu$ (ensuring it decreases), and obtainingeach search direction as the solution of the Newton system\begin{equation} \label{eq:5}\begin{pmatrix}        Q & -A^{T} & 0      \\A & 0 & -I      \\0 & S & V\end{pmatrix}\pmat{  \Delta y\\  \Delta v\\  \Delta s}=-{\pmat{r_d\\r_p\\r_\mu}},\qquad\begin{pmatrix}y^{+}\\v^{+}\\s^{+}\end{pmatrix}=\pmat{y\\v\\s}+\alpha\pmat{\Delta y\\\Delta v\\\Delta s}.\end{equation}The steplength $\alpha$ is chosen to ensure that $(v^+,s^+)$ remainin the strict interior of the cone.One approach to solving for the Newton direction is to apply blockGaussian elimination to~\eqref{eq:5}, and obtain the search directionvia the following systems:\begin{subequations}\begin{align}   (Q+A^T S^{-1}VA)\Delta x & =r_d+A^T S^{-1}(V r_p+r_\mu),                             \label{eq:newton-sc}\\ \Delta v & =S^{-1}(Vr_p+r_\mu-VA\Delta x),\\ \Delta s & =V^{-1}\left(r_\mu-S\Delta v\right).\end{align}\end{subequations}In practice, the matrices $S$ and $V$ are rescaled at each iterationin order to yield search directions with favorable properties. Inparticular, the Nesterov-Todd rescaling redefines $S$ and $V$ so that$S V^{-1} = \mathbf{block}(u)$ for some vector $u$, where\begin{equation} \label{eq:block-diag-def}  \mathbf{block}(u)_i =  \begin{cases}    \mathbf{diag}(u_i)                      & \mbox{if $\mathcal{K}_{i} = \Re_+^{m_i}$,}  \\(2 u_i u_i^T- [u_i^T Ju_i] J)^2 & \mbox{if $\mathcal{K}_{i} = \mathcal{Q}^{m_i}$,}  \end{cases}  \qquad  J = \pmat{1 & 0\\0 & -I_{(m_i-1)}}.\end{equation}The cost of the overall approach is therefore determined by the cost ofsolving, at each iteration, linear systems with the matrix\label{InteriorL}\begin{equation}\label{eq:mathcalL}\mathcal{L}(u) := Q+A^T \mbox{\bf{block}}(u)^{-1} A,\end{equation}which now defines the system~\eqref{eq:newton-sc}.\section{Evaluating the proximal operator} \label{sec:eval-prox-oper}We now describe how to use Proposition~\ref{prop:prox-to-qp} totransform a proximal operator~\eqref{eq:2} into a conic QP that can besolved by the interior algorithm described in~\S\ref{InteriorL}. Inparticular, to evaluate $\mathbf{prox}{H}{g}(x)$ we solve the conicQP~\eqref{eq:qp} with the definitions\begin{equation} \label{eq:Qc} Q := BH^{-1}B^T, \qquad c := d + Bx;\end{equation}the other quantities $A$, $b$, and the cone $\mathcal{K}$, appearverbatim. Algorithm~\ref{algo:proxAlgo} summarizes the procedure.  Aswe note in \S\ref{InteriorL}, the main cost of this procedure isincurred in Step~1, which requires repeatedly solving linear systemsthat involve the linear operator~\eqref{eq:mathcalL}. Togetherwith~\eqref{eq:Qc}, these matrices have the form\begin{equation} \label{eq:L-def} \mathcal{L}(u)=BH^{-1}B^{T}+A^T \mathbf{block}(u)^{-1} A.\end{equation}\begin{algorithm}[t]  \smallskip{}  \begin{tabular}{@{}l@{\ }l}    \textsc{Input} &: $x$, $H$, and QS function $g$ as defined by parameters                             $A$, $b$, $d$, $B$, $\mathcal{K}$    \\[4pt]    \text{\sc Output}&: $\mathbf{prox}_H^g(x)$  \end{tabular}  \smallskip{}\smallskip{}  \textbf{Step 1}: Apply interior method to QP~\eqref{eq:dual-prob} to  obtain $y^*$.  \smallskip{}  \textbf{Step 2}: Return $H^{-1}(c-B^T y^*)$.\smallskip{}  \caption{Evaluating $\mathbf{prox}_H^g(x)$}  \label{algo:proxAlgo}\end{algorithm}Below we offer a tour of several examples, ordered by level ofsimplicity, to illustrate the details involved in the application ofour technique.  The Sherman-Woodbury (SW) identity\[ (D + UMU^T)^{-1} = D^{-1} - D^{-1} U(M^{-1}+U^T D^{-1} U)^{-1} U^T D^{-1},\]valid when $M^{-1}+U^T D^{-1} U$ is nonsingular, proves useful fortaking advantage of certain structured matrices that arise whensolving~\eqref{eq:L-def}. Some caution is needed, however, becauseit is known that the SW identity can be numerically unstable\cite{yip:1986}.For our purposes, it is useful to think of the SW formula as a routinethat takes the elements ($D,U,M$) that define a linear operator$D + UMU^T$, and returns the elements ($D_1, U_1, M_1$) that definethe inverse operator $D_1 +U_1M_1U_1^T = (D + UMU^T)^{-1}$. We assumethat $D$ and $M$ are nonsingular. Algorithm~\ref{alg:sw-formula} summarizesthe operations needed to compute the elements of the inverse operator.\smallskip\begin{algorithm}[t]  \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}  \SetKwProg{Fn}{function}{}{end}  \SetAlgoNoLine  \DontPrintSemicolon  \Fn{\tt SWinv($D, U, M$)}  {    \nl $D_1 \gets D^{-1}$\;    \nl $U_1 \,\gets D_1 U$\;    \nl $M_1 \gets (M^{-1} + U^T U_1)^{-1}$\;    \nl\KwRet{$D_1$, $U_1$, $M_1$}  }  \caption{Inverse via the Sherman-Woodbury identity}  \label{alg:sw-formula}\end{algorithm}\smallskip\noindent Typically, $D$ is a structured operator that admits a fastalgorithm for solving linear systems with any right-hand side, and $U$and $M$ are stored explicitly as densematrices. Step~1 computes a new operator $D_1$ that simply interchangesthe multiplication and inversion operations of $D$. Step~2 applies theoperator $D_1$ to every column of $U$ (typically a tall matrix withfew columns). Step~3 requires inverting a small matrix.\begin{example}[1-norm regularizer; cf.\@ Example~\ref{L1_Example}]\label{L1_Example-cont}Example~\ref{L1_Example} gives the QS representation for$g(x)=\|x\|_1$, and the required expressions for $A$, $B$, and$\mathcal{K}$. Because $\mathcal{K}$ is the nonnegative orthant,$\mathbf{block}(u) =  \mathbf{diag}(u)$; cf.~\eqref{eq:block-diag-def}. With thedefinitions of $A$ and $B$, the linear operator $\mathcal{L}$in~\eqref{eq:L-def} simplifies to\begin{align*}  \mathcal{L}(u) = H^{-1}+A^T \mathbf{diag}(u)A           = H^{-1}+\Sigma,\end{align*}where $\Sigma$ is a positive-definite diagonal matrix that depends on$u$. If it happens that the preconditioner $H$ has a special structuresuch that $H+\Sigma^{-1}$ is easily invertible, it may be convenient toapply the SW identity to obtain equivalent formulas for the inverse\[  \mathcal{L}(u)^{-1} = (H^{-1}+\Sigma)^{-1} = H - H (H+\Sigma^{-1})^{-1} H.\]Banded, chordal, and diagonal-plus-low-rank matrices are examples ofspecially structured matrices that make one of these formulas for$\mathcal{L}^{-1}$ efficient. They yield the efficiency because subtractingthe diagonal matrix $\Sigma$ preserves the structure of either $H$ or$H^{-1}$.\end{example}In the important special case where $H=\mathbf{diag}(h)$ is diagonal,each component~$i$ of the proximal operator for the 1-norm can beobtained directly via the formula\[ [\mathbf{prox}^H_g(x)]_i =\mathbf{sign}(x_i)\cdot\max\{|{x_i}-1|/h_{ii},\, 0\},\]where $h_{ii}$ are the diagonal elements of $H$. This corresponds tothe well-known soft-thresholding operator. No simple formula exists,however, for more general matrices.\begin{example}[Graph-based 1-norm]  \label{ex:graph-1-norm}  Consider the graph-based 1-norm function from  Example~\ref{ex:graph1-norm-example} induced by a graph $\mathcal{G}$  with adjacency matrix $N$. Substitute the definitions of $A$ and  $B$ from \eqref{eq:qs-rep-graph-1-norm} into the formula for $\mathcal{L}$  and simplify to obtain  \begin{equation*}    \mathcal{L}(u) = NH^{-1} N^T + A^T \mathbf{diag}(u)A = NH^{-1} N^T + \Sigma,  \end{equation*}  where $\Sigma:=A^T\mathbf{diag}(u)A$ is a positive-definite diagonal  matrix. (As with Example~\ref{L1_Example-cont}, $\mathcal{K}$ is the  positive orthant, and thus $\mathbf{block}(u) =  \mathbf{diag}(u)$.)  Linear systems of the form $\mathcal{L}(u)p=q$ then can be solved with the  following sequence of operations outlined in  Algorithm~\ref{algo:graph-1-norm}, in which we assume that  $H=\Lambda+UMU^T$, where $\Lambda$ is diagonal.  \begin{algorithm}    \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}    \DontPrintSemicolon    \nl$(\Lambda_1, U_1, M_1) \gets \SWinv(\Lambda, U, M)$    \Comment*{$H^{-1} \equiv \Lambda_1 + U_1M_1U_1^T$}    \nl$\Sigma_1 \gets N\Lambda_1N^T + \Sigma$\;    \nl$(\Sigma_2, U_2, M_2) \gets \SWinv(\Sigma_1, NU_1, M_1)$    \Comment*{$\mathcal{L}(u)^{-1}\equiv \Sigma_2+U_2M_2U_2^T$}    \nl$p \gets \Sigma_2 q + U_2 M_2 U_2^T q$ \Comment*{solve      $\mathcal{L}(u)p=q$} \nl\KwRet{$p$}    \caption{Solving the system $\mathcal{L}(u)p=q$ for the graph-based 1-norm.}    \label{algo:graph-1-norm}  \end{algorithm}  \noindent Observe from the definition of $H$ and the definition of  $\Sigma_1$ in Step~2  that  \[    \mathcal{L}(u) = \Sigma_1 + NU_1 M_1 U_1^T N^T,  \]  and then Step~3 computes the quantities that define the inverse of  $\mathcal{L}$.  The bulk of the work in the above algorithm happens in  Step~3, where $\Sigma_2\equiv\Sigma_1^{-1}$ is applied to each  column of $NU_1$ (see Step~2 of the \SWinv\ function), and in  Step~4, where $\Sigma_2$ is applied to $q$.  Below we give two  special cases where it is possible to take advantage of the  structure of $N$ and $H$ in order to apply $\Sigma_2$ efficiently to  a vector.  \smallskip\begin{description}\item[1-dimensional total variation.] Suppose that the graph $\mathcal{G}$  is a path. Then the $(n-1)\times n$ adjacecy matrix is given by  \[    N =  \pmat{-1 & 1\\ & \ddots& \ddots\\&&-1 & 1}.  \]  The matrix $\Sigma_1:=N\Lambda^{-1} N^T+\Sigma$ (see Step~2 of the  above algorithm) is tridiagonal, and hence equations of the form  $\Sigma_1q = p$ can be solved efficiently using standard techniques,  e.g., \cite[Algorithm 4.3.6]{GoluLoan:1989}.%  In higher dimensions, if the boundary conditions% are toroidal, the matrix can be similarly diagonalized with a% $n$-dimensional Fourier% transform.% For example, in the 2-dimensional case,% \[% L = L_c \otimes I_2 + I_2 \otimes L_c \qquad \mbox{$L_c$ is the Laplacian of a loop,}% \]% which is block-circulant. If $F$ is the discrete Fourier transform,% \[% (F \otimes F)^{-1}(L_c \otimes I_2 + I_2 \otimes L_c) =% (F^{-1}L_{c})\otimes(F^{-1}I_{2})+(F^{-1}I_{2})\otimes(F^{-1}L_{c}).% \]% Because all four terms are diagonal, the 2-dimensional Fourier% transform diagonalizes $L$.\smallskip\item[Chordal graphs.] If the graph $\mathcal{G}$ is chordal, than the  matrix $N^T D N$ is also chordal when $D$ is diagonal. This implies  that it can be factored in time linear with the number of edges of  the graph \cite{Andersen:2010}. We can use this fact to apply  $\Sigma_2\equiv\Sigma_1^{-1}$ efficiently, as follows: let  $(\Sigma_3, U_3, M_3)=\SWinv(\Sigma, N, \Lambda_1)$, which implies  \[          \Sigma_2 := \Sigma_3 + U_3M_3U_3^T,    \quad\mbox{where}    \quad \Sigma_3 := \Sigma^{-1},    \quad M_3 := (N^T\Sigma^{-1} N + \Lambda_1)^{-1}.  \]  Because $N^T\Sigma^{-1} N$ is chordal, so is $M_3$, and any methods  efficient for solving with chordal matrices can be used when  applying $\Sigma_2$.\end{description}\end{example}\begin{example}[1-norm constraint; cf.\@ Example~\ref{ex:1-norm-ball}]\label{ex:1-norm-ball-v2}Example~\ref{ex:1-norm-ball} gives the QS representation for theindicator function on the 1-norm ball. Because the constraints on $y$in~\eqref{eq:1-norm-indicator} involve only bound constraints,$\mathbf{diag}(u)=\mathbf{diag}(u)$. With the definitions of $A$ and $B$ fromExample~\ref{ex:1-norm-ball}, the linear operator $\mathcal{L}$ has the form\[\mathcal{L}(u)=\begin{pmatrix}0\\I_n\end{pmatrix}H^{-1}\begin{pmatrix} 0 & I_n\end{pmatrix}+\begin{pmatrix}\phantom-\mathbf{1}_n^T & \mathbf{1}_n^T \\ -I_n & I_n\end{pmatrix}\begin{pmatrix}\mathbf{diag}(u_{1})\\ & \mathbf{diag}(u_{2})\end{pmatrix}\begin{pmatrix}\mathbf{1}_n&-I_n\\\mathbf{1}_n&\phantom-I_n\end{pmatrix},\]where $u = (u_1, u_2)$. Thus, $\mathcal{L}$ simplifies to\[  \mathcal{L}(u)  = \begin{pmatrix}      \mathbf{1}_n^T u & (u^{-})^T   \\ u^{-}  & H^{-1} + \Sigma    \end{pmatrix}    \text{\normalfont where}    \Sigma:=\mathbf{diag}(u^{+}),    \quad    \begin{array}{c}      u^{+}:=\phantom-u_{1}+u_{2},\\      u^{-}:=-u_{1}+u_{2}.    \end{array}\]Systems that involve $\mathcal{L}$ can be solved by pivoting on the block$(H^{-1}+\Sigma)$. The cases where this approach is efficient areexactly those that are efficient in the case ofExample~\ref{L1_Example-cont}.\end{example}\begin{example}[2-norm; cf.\@ Example~\ref{ex:2-norm}]  \label{ex:group-lasso-cont}  Example~\ref{ex:2-norm} gives the QS representation for the 2-norm  function. Because $\mathcal{K}=Q^n$, then $\mathbf{block}(u)=(2uu^T-[u^T Ju]J)^2$,  where $u = (u_0, \bar u)$ and $J$ is specified  in~\eqref{eq:block-diag-def}. With the expressions for $A$ and $B$  from Example~\ref{ex:2-norm}, the linear operator $\mathcal{L}$ reduces to  \begin{equation} \label{eq:lin-op-sum-of-squares}%   \mathcal{L}(u) = H^{-1}+(u^{T}Ju)^{2}I_n+8u_0\bar{u}\bar{u}^{T}.    \mathcal{L}(u) = H^{-1}+\alpha I_n + vv^T,    \text{with}    \alpha = (u^T J u)^2,\ v=\sqrt{8u_0}\cdot\bar u.  \end{equation}  This amounts to a perturbation of $H^{-1}$ by a multiple of the  identity, followed by a rank-1 update. Therefore, systems that  involve $\mathcal{L}$ can be solved at the cost of solving systems with  $H + \alpha I_n$ (for some scalar $\alpha$).  Of course, the proximal map of the 2-norm is easily computed by  other means; our purpose here is to use this as a building block for  more useful penalties, such as Example~\ref{ex:sum-of-norms}, which  involves the sum-of-norms function shown in  \eqref{eq:sum-of-norms-g}.  Suppose that the $p$ partitions do not  overlap, and have size $n_i$ for $i=1,\ldots,p$.  The operator  $\mathcal{L}$ in~\eqref{eq:lin-op-sum-of-squares} generalizes to  \begin{equation*}    \mathcal{L}(u) = H^{-1}+    \underset{W}{\underbrace{    \begin{pmatrix}      \alpha_{1}I_{n_1}+v^{1}(v^{1})^{T}\\     & \ddots\\     &  & \alpha_{p}I_{n_p}+v^{p}(v^{p})^{T}    \end{pmatrix}    }}    \ ,\quad    \begin{aligned}      u^{i}  & =(u^i_{0},\bar{u}^{i})\\      \alpha_{i}  &= (u^{iT} Ju^{i})^2\\      v^{i} & =\sqrt{8u^i_{0}}\cdot \bar{u}^{i},    \end{aligned}  \end{equation*}  where each vector $u_i$ has size $n_i+1$.  When $p$ is large, we can treat each diagonal block of $W$ as an  individual (small) diagonal-plus-rank-1 matrix. If $H^{-1}$ is  diagonal-plus-low-rank, for example, the diagonal part of $H^{-1}$  can be subsumed into $W$. In that case, each diagonal block in $W$  remains diagonal-plus-rank-1, which can be inverted in parallel by  handling each block individually. Subsequently, the inverse of  $\mathcal{L}$ can be obtained by a second correction.  Another approach, when $p$ is small, is to consider $W$ as a  diagonal-plus-rank-$p$ matrix:  \[    W =    \begin{pmatrix}    \alpha_{1}I_{n_1}\\     & \ddots\\     &  & \alpha_{p}I_{n_p}    \end{pmatrix}    +\begin{pmatrix}      v^{1}\\ & \ddots\\ &  & v^{p}    \end{pmatrix}    \begin{pmatrix}      v^{1}\\      & \ddots\\      &  & v^{p}    \end{pmatrix}^T.  \]  This representation is convenient: systems involving $\mathcal{L}$  can be solved efficiently in a manner identical to that of  Example~\ref{L1_Example-cont} because $W$ is a  diagonal-plus-low-rank matrix.\end{example}\begin{example}[separable QS functions]  \label{ex:separable} Suppose that $g$ is separable, i.e.,\[ g(x)=\gamma(x_1) + \cdots + \gamma(x_n),\]where $\gamma:\Real\to\Real$ is a QS function with parameters$(A_\gamma,b_\gamma,B_\gamma,d_\gamma,\Real_+^{np})$, and $p$ is aninteger parameter that depends on $\gamma$. The parameters $A$ and $B$for $g$ follow from the concatenationrule~\eqref{Calculus-Rule-Concat}, and $A=(I_n\otimes A_\gamma)$ and$B=(I_n\otimes B_\gamma)$. Thus, the linear operator $\mathcal{L}$ is givenby\begin{align*}  \mathcal{L}(u) =  (I_n \otimes B_\gamma)H^{-1}(I_n \otimes B_\gamma)^{T}  + (I_n \otimes A_\gamma)^{T}\mathbf{diag}(u)(I_n\otimes A_\gamma).\end{align*}Apply the SW identity to obtain\[\mathcal{L}(u)^{-1} =\Lambda^{-1}-\Lambda^{-1}(I_n\otimes B_\gamma)(H+\Sigma)^{-1}(I_n\otimes B_\gamma)^{T}\Lambda^{-1},\]where $\Lambda=\mathbf{diag}(\Lambda_1,\ldots,\Lambda_n)$,\begin{align*}  \Lambda_i =  A_\gamma^T\mathbf{diag}(u^i)A_\gamma,  \quad\text{and}\quad  \Sigma = \mathbf{diag}(B_\gamma^{T}\Lambda_1^{-1}B_\gamma,\dots,B_\gamma^{T}\Lambda_n^{-1}B_\gamma).\end{align*}Because the function $\gamma$ takes a scalar input, $B_\gamma$ is avector. Hence $\Sigma$ is a diagonal matrix. Note too that $\Lambda$is a block diagonal matrix with $n$ blocks each of size $p$. We canthen solve the system $\mathcal{L}(u)p = q$ with the following steps:\vspace{3mm}\RestyleAlgo{plain}\begin{algorithm}[H]  \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}  \DontPrintSemicolon \nl  $q_1 \gets (I_n \otimes B_\gamma)^T\Lambda^{-1}q$ \; \nl  $q_2 \gets (H + \Sigma)^{-1}q_1$ \; \nl  $q_3 \gets \Lambda^{-1}q_2 -\Lambda^{-1}(I_n\otimes B_\gamma)q_2$\end{algorithm}\vspace{3mm}\noindent The cost of solving systems with the operator $\mathcal{L}$ is dominated bysolves with the block diagonal matrix $\Lambda$ (Steps~1 and~3) and$H + \Sigma$ (Step~2). The cost of the latter linear solve is exploredin Example~\ref{L1_Example-cont}.\end{example}\section{A proximal quasi-Newton method} \label{sec:quasi-Newton}We now turn to the proximal-gradient method discussed in \S\ref{introduction}. Our primary goal is to demonstrate the feasibility of the interior approach for evaluating proximal operators of QS functions. A secondary goal is to illustrate how this technique leads to an efficient extension of the quasi-Newton method for nonsmooth problems of practical interest.% We implement a limited-memory BFGS (L-BFGS) method, which generates a sequence of matrices $\{H_k\}$ that approximate the Hessian $\nabla^2 f(x\k)$ at each iterate $x\k$.  We follow \cite{Scheinberg2016} and implement a limited-memory BFGS(L-BFGS) variant of the proximal-gradient method that has nolinesearch and that approximately evaluates the proximaloperator. Scheinberg and Tang establish a sublinear rate ofconvergence for this method when the Hessian approximations aresuitably modified by adding a scaled identity matrix, and when thescaled proximal maps are evaluated with increasing accuracy. In theirproposal, the accuracy of the proximal evaluation is based on boundingthe value of the approximation to \eqref{eq:4}. We depart from thiscriterion, however, and instead use theresidual~\eqref{eq:int_residual} obtained by the interior solver todetermine the required accuracy. In particular, we require that theoptimality criterion of the interior algorithm used to evaluate theoperator is a small multiplicative constant $\kappa$ of the currentoptimality of the outer proximal-gradient iterate, i.e.,\[\|R_\mu(y,v,s)\| \leq \kappa \| x_k - \mathbf{prox}_g(x_k-\nabla  f(x_k)) \|.\]This heuristic is reminiscent of the accuracy required of the linearsolves used by an inexact Newton method for rootfinding~\cite{DembEiseStei:1982}.  Note that the proximal map$\mathbf{prox}_g \equiv \mathbf{prox}^{I}_{g}$ used above is unscaled, which in manycases can be easily computed when $g$ is seperable. %  is related to the approaches advocated by several% authors.  Schmidt et al.~\cite{schmidt2009optimizing} consider an% L-BFGS projected-gradient method, and use a first-order method for the% projection evaluation, which ignores the structure in the Hessian% approximation.  On the other hand, Scheinberg et% al.~\cite{Scheinberg2016} and Zhong et al.~\cite{NIPS2014_5384}% consider an L-BFGS proximal method, but target 1-norm regularization% and a coordinate-descent method for evaluating the scaled proximal% operator.  Our implementation of the proximal-gradient% iteration is most closely related to Scheinberg et al. in the sense% that we also do not imple Finally, unlike \cite{byrd2013inexact}, we% do not perform a line-search, and we choose our stopping criteria such% that the residual from the interior algorithm is a multiplicative% constant of the generalized residual,% $$\|R_\mu(y,v,s)\| \leq \kappa \norm{x_k - \prox{}{g}(x_k-\nabla% f(x_k))},$$ % where $R_\mu$ is the residual of the interior point method% in \eqref{eq:int_residual}. Though there is no formal guarantee of% convergence of this algorithm, this section exists merely to% demonstrate the efficacy of our proximal method on real world% datasets.\subsection{Limited-memory BFGS updates} \label{sec:L-BFGS}Here we give a brief outline the L-BFGS method forobtaining Hessian approximations of a smooth function $f$. We followthe notation of \cite[\S6.1]{NoW99}, who use $H_k$ to denote thecurrent approximation to the \emph{inverse} of the Hessian of $f$. Let$x_k$ and $x_{k-1}$ be two consecutive iterates, and define the vectors\[  s_{k} = x_{k+1}-x_{k},  \quad\text{and}\quad  y_{k} = \nabla f(x_{k+1})-\nabla f(x_{k}).\]A ``full memory'' BFGS method updates the approximation $H_k$ viathe recursion\begin{align*}  H_{0}=\sigma I,  \qquad  H_{k+1} = H_{k}-\frac{H_{k}s_{k}s_{k}^{T}H_{k}}{s_{k}^{T}H_k s_k}+\frac{y_{k}y_{k}^{T}}{y_{k}^{T}s_{k}},\end{align*}for some positive parameter $\sigma$ that defines the initialapproximation. The limited-memory variant of the BFGS update (L-BFGS)maintains the most recent $m$ pairs $(s_k,y_k)$, discarding oldervectors. In all cases, $m\ll n$, e.g., $m = 10$. The globalizationstrategy advocated by \cite{Scheinberg2016} may add a small multipleof the identity to $H_k$. This modification takes the place of apotentially expensive linesearch, and the correction is increased ateach iteration if a certain condition for decrease is not satisfied.Each interior iteration for evaluating the proximal operator dependson solving linear systems with $\mathcal{L}$ in~\eqref{eq:L-def}. In all ofthe experiments presented below, each interior iteration has a costthat is linear in the number of variables $n$.\section{Numerical experiments}We have implemented the proximal quasi-Newton method as a Juliapackage \cite{BEKS14}, called $\mathtt{QSip}$ designed for problems of theform~\eqref{eq:6}, where $f$ is smooth and $g$ is a QS function. Thecode is available at the URL\begin{center}  \url{https://github.com/MPF-Optimization-Laboratory/QSip.jl}\end{center}A primal-dual interior method, based on ideas from the CVXOPT softwarepackage \cite{Andersen:2010}, is used forAlgorithm~\ref{algo:proxAlgo}. We consider below several examples. Thefirst three examples apply the $\mathtt{QSip}$ solver to minimize benchmarkleast-squares problems with different nonsmooth regularizers that areQS representable; the last example applies the solver to a sparselogistic-regression problem on a standard data set.\subsection{Timing the proximal operator} \label{sec:timeprox}The examples that we explored in \S\ref{sec:eval-prox-oper} have afavorable structure that allows each interior iteration for evaluatingthe proximal map $\mathbf{prox}^{H}_{g}(x)$ to scale linearly with problemsize. In this section we verify this behavior empirically for problemswith the structure\begin{equation} \label{eq:proxtime}H = I + UU^T, \qquad g(x) = \|x\|_1, \qquad U \in \Real^{n \times k}\end{equation}for different values of $k$ and $n$.  This choice ofdiagonal-plus-low-rank matrices is designed to mimic the structure ofmatrices that appear in L-BFGS. Here $U$ and $x$ are chosen withrandom normal entries. As described in Example~\ref{L1_Example}, thesystem $\mathcal{L}(u)$ is inverted in linear time using the SWidentity.We evaluate the proximal map on $100$ random instances for eachcombination of $k$ and $n$, and plot in Figure 1 the average timeneeded to reach an accuracy of $10^{-7}$, as measured by theoptimality conditions in the interior algorithm.  Because in practicethe number of iterations of the interior method is almost independentof the size of the problem, the time taken to compute the proximal mapis a predictable, linear function of the size of the problem.\begin{figure}\centering\includegraphics[width=0.65\textwidth]{prox/proxtime-crop.pdf}\caption{Time taken to compute $\mathbf{prox}^{H}_{g}(x)$ versus $n$, for  $k=1, 10, 100$; see~\eqref{eq:proxtime}.}\end{figure}\subsection{Synthetic least-square problems}The next set of examples all involve the least-squares objective\begin{equation}\label{eq:ls-objective}  f(x) = \tfrac{1}{2}\|Ax-b\|^2_2.\end{equation}Two different procedures are used to construct matrices $A$, asdescribed in the following sections. In all cases, we follow thetesting approach described by \cite{6399612} for constructing a testproblem with a known solution: fix a vector $x^{\star}$ and choose$b=Ax^{\star}-A^{-T}v$, where $v\in\partial g(x^\star)$. Note that\[\partial(f+g)(x^\star)=A^{T}(Ax^\star-[Ax^{\star}-A^{-T}v])+\partial g(x^{\star})=\partial g(x^{\star})-v.\]Because $v\in \partial g(x^\star)$, the above implies that$0\in\partial(f+g)(x^\star)$, and hence $x^\star$ minimizes theobjective $f+g$.  In the next three sections, we apply $\mathtt{QSip}$ in turnto problems with $g$ equal to the 1-norm, the group LASSO (i.e., sumof 2-norm functions), and total variation.\subsubsection{One-norm regularization} \label{sec:1-norm-reg}In this experiment we choose $g = \|\cdot\|_1$, which gives the 1-normregularized least-squares problem, often used in applications ofsparse optimization. Following the details in Example\ref{L1_Example}, the system $\mathcal{L}(u)$ is adiagonal-plus-low-rank matrix, which we invert using the SW identity.The matrix $A$ in~\eqref{eq:ls-objective} is a 2000-by-2000 lowertriangular matrix with all nonzero entries equal to $1$. The bandwidth$p$ of $A$ is adjustable, and determines its coherence\[\mbox{coherence}(A)=\max_{i\neq j}\frac{a_i^T a_j}{\|a_i\|\|a_j\|} = \sqrt{\frac{p-1}{p}},\]where $a_i$ is the $i$th column. As observed by \cite{6399612}, thedifficulty of 1-norm regularized least-squares problems are stronglyinfluenced by the coherence. Our experiments use matrices $A$ withbandwidth $p=500, 1000, 2000$.Figure~\ref{Exp_L1} shows the results of applying the $\mathtt{QSip}$ solverwith a memories $k=1,10$, labeled ``QSIP mem $= k$''. We also considercomparisons against two competitive proximal-based methods. The firstis a proximal-gradient algorithm that uses the Barzilai-Borweinsteplength \cite{BarzBorw:1988,wright2009sparse}.  This is our ownimplementation of the method, and is labeled ``Barzilai-Borwein'' inthe figures. The second is the proximal quasi-Newton methodimplemented by \cite{NIPS2012_4523}, which is based on asymmetric-rank-1 Hessian approximation; this code is labeled``PG-SR1''. The $\mathtt{QSip}$ solver with memory of 10 outperforms the othersolvers. The quasi-Newton approximation benefits problems with highcoherence ($p$ large) more than problems with low coherence ($p$small). In all cases, the experiments reveal that the additional costinvolved in evaluating a proximal operator (via an interior method) isbalanced by the overall cost of the algorithm, both in terms ofiterations (i.e., matrix-vector products with $A$) and time.\begin{figure}[t]  \centering  \begin{tabular}{@{}c@{\ }c@{}}   \includegraphics[width=0.44\textwidth]{prox/tang_results_L1-crop}  &\includegraphics[width=0.44\textwidth]{prox/tang_results_L1_iter-crop} \\\includegraphics[width=0.44\textwidth]{prox/tang_results_L1_1000-crop}  &\includegraphics[width=0.44\textwidth]{prox/tang_results_L1_1000_iter-crop} \\\includegraphics[width=0.44\textwidth]{prox/tang_results_L1_500-crop}  &\includegraphics[width=0.44\textwidth]{prox/tang_results_L1_500_iter-crop}  \end{tabular}  \caption{Performance of solvers applied to 1-norm regularized    least-squares problems of increasing difficulty. The left and    right columns, respectively, track the distance of the current    solution estimate to the true solution versus time and iteration    number.  Top row: $p=2000$ (highest coherence); middle row:    $p=1000$; bottom row: $p=500$ (lowest coherence).       \label{Exp_L1}}\end{figure}\subsubsection{The effect of conditioning}\label{sec:conditioning}It is well known that the proximal-gradient method converges slowlyfor ill conditioned problems. The proximal L-BFGS method may help toimprove convergence in such situations. We investigate the observedconvergence rate of the proximal L-BFGS approach on a family ofleast-squares problems with 1-norm regularization with varying degreesof ill conditioning. For these experiments, we take $A$in~\eqref{eq:ls-objective} as the 2000-by-2000 matrix\[A = \alpha_L \begin{pmatrix} T & 0\\0 & 0\end{pmatrix} + \alpha_\mu I,\]where $T$ is a 1000-by-1000 tridiagonal matrix with constant diagonalentries equal to 2, and constant sub- and super-diagonal entries equalto ${-}1$. The parameter $\alpha_L/\alpha_\mu$ controls theconditioning of $A$, and hence the conditioning of the Hessian $A^T A$of $f$.We run L-BFGS with 4 different memories (``mem''): $0$ (i.e., proximalgradient with a Barzilai-Borwein steplength), $1$, $10$, and $100$.We terminate the algorithm either when the error drops beneath$10^{-8}$, or the method reaches $10^3$ iterations. Our method ofmeasuring the {observed convergence} (OC) computes the line of bestfit to the log of optimality versus $k$, which results in the quantity\[\mbox{Observed Convergence}:= \frac{\sum_{k=0}^{N}k\cdot\log \|x_{k}-x_{*}\|}{\sum_{k=0}^{N} \log \|x_{k}-x_{*}\|},\]where $N$ is the total number of iterations.The plot in Figure~\ref{fig:convergence} shows the ratio of the OC forL-BFGS relative to the observed convergence of proximal gradient(PG). This quantity can be interpreted the amount of work that asingle quasi-Newton step performs relative to the number of PGiterations. The plot reveals that the quasi-Newton method is faster atall condition numbers, but is especially effective for problems withmoderate conditioning. Also, using a higher quasi-Newton memory almostalways lowers the number of iterations. This benefit is mostpronounced when the problem conditioning is poor.Together with \S\ref{sec:timeprox}, this section gives a broad pictureof the trade-off between the proximal quasi-Newton and proximalgradient methods. The time required for each proximal gradientiteration is dominated by the cost of the gradient computation becausethe evaluation of the unscaled proximal operator is often trivial. Onthe other hand, the proximal quasi-Newton iteration additionallyrequires evaluating the scaled proximal operator.  Therefore, theproximal quasi-Newton method is most appropriate when this cost issmall relative to the gradient evaluation.\begin{figure}\centering\includegraphics[width=0.65\textwidth]{prox/cond-crop}\caption{Performance of the proximal quasi-Newton method relative to  proximal gradient for problems of varying condition number.}\label{fig:convergence}\end{figure}% paragraph paragraph_name (end)\subsubsection{Group LASSO}Our second experiment is based on the sum-of-norms regularizerdescribed in Examples~\ref{ex:sum-of-norms}and~\ref{ex:group-lasso-cont}. In this experiment, the $n$-vector(with $n=2000$) is partitioned into $p=5$ disjoint blocks of equalsize. The matrix $A$ is fully lower triangular.Figure~\ref{Exp_L2} clearly shows that the $\mathtt{QSip}$ solver outperformsthe PG method with the Barzilai-Borwein step size.  Although werequired $\mathtt{QSip}$ to exit with a solution estimate accurate within 6digits (i.e., $\log\|x-x^*\| \le 10^{-6}$), the interior solverfailed to achieve the requested accuracy because of numericalinstability with the SW formula used for solving the Newtonsystem. This raises the question of how to use efficient alternativesto the SW update that are numerically stable and can still leveragethe structure of the problem.\begin{figure}  \centering  \includegraphics[width=0.65\textwidth]{prox/tang_results_GL-crop}  \caption{Performance of solvers applied to a group-Lasso problem.    The horizontal axis measures elapsed time; the vertical axis    measures distance to the solution.\label{Exp_L2}}\end{figure}\subsubsection{1-dimensional total variation} \label{sec:TV} Our thirdexperiment sets\[g(x)=\sum_{i=1}^{n-1}|x_{i+1}-x_{i}|,\]which is the anisotropic total-variation regularizer described inExamples~\ref{ex:graph1-norm-example} and~\ref{ex:graph-1-norm}. Thematrix $A$ is fully lower triangular. Figure~\ref{Exp_TV} compares theconvergence behavior of $\mathtt{QSip}$ with the Barzilai-Borwein proximalsolver. The Python package {\tt  prox-tv}~\cite{conf/icml/Barbero11,barberoTV14} was used for theevaluation of the (unscaled) proximal operator, needed by theBarzilai-Borwein solver. The $\mathtt{QSip}$ solver, with memories of 1 and 10,outperformed the Barzilai-Borwein solver.\begin{figure}  \centering  \includegraphics[width=0.65\textwidth]{prox/tang_results_TV-crop}  \caption{Performance of the $\mathtt{QSip}$ solver applied to a 1-dimensional    total-variation problem.} \label{Exp_TV}\end{figure}\subsection{Sparse logistic regression}This next experiment tests $\mathtt{QSip}$ on the sparse logistic-regressionproblem problem\[  \underset{x}{\mbox{minimize}} \quad  \frac{1}{N}\sum_{i=1}^{N}\log(1+\exp[a_{i}^T x])+\lambda \|x\|_{1},\]where $N$ is the number of observations.  The\textit{Gisette}~\cite{guyon2004result} and\textit{Epsilon}~\cite{PascalChallenge2016} datasets, standardbenchmarks from the UCI Machine LearningRepository~\cite{Lichman:2013}, are used for the feature vectors$a_i$.% In the Gisette Dataset, Each% vector $a_i$ represents a handwritten digit (either ``4'' or ``9'') and the% logistic-regression problem builds a classifier that attempts to distinguish% between the two digits. % The co-variate vectors $a_i$ in this Gisette set are% dense, and therefore the gradient is costly to compute.\textit{Gisette} has $5K$ parameters and $13.5K$ observations;\textit{Epsilon} has $2K$ parameters with $400K$ observations. Thesedatasets were chosen for their large size and modest number ofparameters. In all of these experiments, $\lambda = 0.01$.Figure~\ref{fig:l1logreg} compares $\mathtt{QSip}$ to the Barzilai-Borweinsolver, and to newGLMNet \cite{KimKohLustBoydGori:2007}, astate-of-the-art solver for sparse logistic regression. (Otherpossible comparisons include the implementation of\cite{Scheinberg2016}, which we do not include because of difficultycompiling that code.) Because we do not know a priori the solution forthis problem, the vertical axis measures the log of the optimalityresidual $\|{x_k - \mathbf{prox}_{g}(x_k-\nabla f(x_k))}\|_\infty$ of thecurrent iterate. (The norm of this residual necessarily vanishes atthe solution.)  On the \textit{Gisette} dataset, Barzilai-Borwein andnewGLMNNet are significantly faster than the proximal quasi-Newtonimplementation. On the \emph{Epsilon} dataset, however, thequasi-Newton is faster at all levels of accuracy.\begin{figure}  \centering  \begin{tabular}{@{}c@{\ }c@{}}   \includegraphics[width=0.485\textwidth]{prox/logistic-crop}  &\includegraphics[width=0.485\textwidth]{prox/logistic_iter-crop} \\\includegraphics[width=0.485\textwidth]{prox/logistic_eps-crop}  &\includegraphics[width=0.485\textwidth]{prox/logistic_eps_iter-crop}  \end{tabular}  \caption{Performance of solvers on a sparse logistic-regression    problem. Top row: \emph{Gisette} dataset; bottom row:    \emph{Epsilon} dataset. The left and right columns, respectively,    track the optimality of the current solution estimate versus    elapsed time and iteration number.}  \label{fig:l1logreg}\end{figure}\section{Conclusion} \label{sec:conclusion}Much of our discussion revolves around techniques for solving theNewton systems~\eqref{eq:5} that arise in the implementation of aninterior method for solving QPs. The Sherman-Woodbury formula featuresprominently because it is a convenient vehicle for taking advantage ofthe structure of the Hessian approximations and the structuredmatrices that typically define QS functions. Other alternatives,however, may be preferable, depending on the application.For example, we might choose to reduce the 3-by-3 matrixin~\eqref{eq:5} to an equivalent symmetrized system\[ \pmat{-Q & A^T \\ A & D}\pmat{\Delta y\\\Delta s}   = -\pmat{-{r_d}\\r_p+V^{-1} r_\mu}\]with $D:=V^{-1} S$. As described by \cite{benzi2008some}, Krylov-basedmethod, such as MINRES \cite{PaigSaun:1975}, may be applied to apreconditioned system, using the preconditioner\[  P = \pmat{ -\mathcal{L}(u)  \\ & D },\]where $\mathcal{L}(u)$ is defined in~\eqref{eq:mathcalL}. This ``ideal''preconditioner clusters the spectrum into three distinct values, sothat in exact arithmetic, MINRES would converge in threeiterations. The application of the preconditioner requires solvingsystems with $\mathcal{L}$ and $D$, and so all of the techniques discussedin~\S\ref{sec:eval-prox-oper} apply. One benefit, however, which we havenot explored here, is that the preconditioning approach allows us toapproximate $\mathcal{L}^{-1}(u)$, rather than to compute it exactly, which mayyield computational efficiencies for some problems.