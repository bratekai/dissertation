%!TEX root = ../Dissertation.tex

We will assume for simplicity that
$\alpha_{k}\equiv 1/L$. We will make the following blanket
assumptions about $f$. First, the solution set $\Soln$ of \eqref{eq:minimize_fpg}
is nonempty. For all $x$ and $y$, there exist  positive constants $L$
and $\tau\geq 1$ such that
\begin{subequations}
\begin{align}
  \|{\nabla f(y)-\nabla f(x) \|} 
  &\le L\,\|{y-x}\|, 
  \label{a:lipschitz}
  \\ \min_{\bar{x}\in\Soln}\,\|x-\bar{x}\|
  &\leq\tau\,\|{x-\mathbf{prox}_{1/L}(x-\tfrac1L\nabla f(x)})\|\qquad 
  \forall x \in \mbox{dom}(g)
   \label{a:natural-res}
\end{align}
\end{subequations}
Assumption~\eqref{a:lipschitz} asserts the Lipschitz continuity of the
gradient of $f$. Assumption~\eqref{a:natural-res} is an error bound on
the generalized residual. This generalized residual has been explored
in local contexts in Tseng and Yun~\cite{Tsy09a} and Luo and
Tseng~\cite{LuT94}; for simplicity our assumption is stronger,
however, requiring the bound to be global.

$\tau$ can be seen as a nonsmooth proxy for the condition number. If
$g\equiv0$ and $f$ is strongly convex with parameter $\mu$, then
\eqref{a:natural-res} holds with $\tau = L/\mu$. More generally, if
$g$ is an indicator function on a polyhedral set and $f$ is strongly
convex, this bound holds globally, with parameter $\tau = (L +
1)/\mu$. \cite{Pan87} Theorem 3.1. Recently, a global version of this
error bound has been developed for non- strongly convex functions
which degrades with the size of the neighborhood see
\cite{wang2014iteration} Theorem 18. We can use such a bound if the
function $g$ is an indicator over a polyhedral set, and $f$ can be
written in the form \[ f(x) = r(Ax) + b^{T}x \] for any $A$, $b$,
and $r$ is strongly convex. If the above two conditions hold, then

% Let $R^k:=\sum_{i=0}^{k-1}\rho^i$, where $\rho<1$ is a constant
% specified in Lemma~\ref{lem:obj-val-bnd} in terms of $L$ and $\tau$.
% Let $\mathcal{F}_k=\sigma(e_1,e_2,\ldots,e_k)$ be the $\sigma$-algebra
% generated by the sequence of errors $e_{i}$. When the context is
% clear, $[z]_{i}$ denotes the $i$th component of a vector~$z$.

\begin{lem}
  \label{lem:obj-val-bnd}
  Let $\pi_k = f(x_k) - \min f(x)$. Then after $k$ iterations of algorithm~\eqref{eq:prox-gradient},
  \begin{equation*}
    \pi_k  \le \rho^k \pi_0 
    + \frac{1}{\vartheta}
    \sum_{i=0}^{k-1}\rho^{k-1-i}\|e_i\|^2,
  \end{equation*}
  where
  $$\rho = 1- \frac{1}{1+40\tau^{2}}\in(0,1) \qquad \text{and} 
  \qquad \vartheta = L\cdot\left(\frac{1}{40\tau^{2}}+1\right)>0.$$
\end{lem}

The proof of this result follows the template laid out by Luo and
Tseng \cite[Theorem~3.1]{luo1993error}, modified to keep the error
term $e_k$ explicit. So~\cite{So:2013} also provides a similar
derivation for the case where $g\equiv0$, in which case it seems
possible to obtain tighter constants $\rho$ and $\vartheta$.  If
additionally $\|e_{k}\|=0$, then the result reduces to the well-known
fact that steepest descent decreases the objective value linearly. The
convergence rate, as expected, is a function of the condition number,
We
note that the constants are invariant to scalings of $f+g$.

\noindent 
\begin{lem}[\bf Three-point property with error] \label{le:3-points}
For all $y\in \text{dom}(g)$,
\begin{align*}
  g(y) &\geq g(x_{k+1})
  +{(\nabla f(x_{k})+e_{k})}^T{x_{k+1}-y}
 +\frac{L}{2}\|x_{k+1}-x_{k}\|^{2}
 +\frac{L}{2}\|y-x_{k+1}\|^{2}-\frac{L}{2}\|y-x_{k}\|^{2}.
\end{align*}
\end{lem}

\begin{proof}
Let $\psi_{k}(x) := g(x)+f(x_{k})+\left\langle \nabla
  f(x_{k})+e_{k},x-x_{k}\right\rangle +\frac{L}{2}\|x-x_{k}\|^{2}$. 
Because $\psi_{k}$ is strongly convex,
\[
  \psi_{k}(y)\geq\psi_{k}(x)
  + {q^T}{y-x}
  + \frac{L}{2}\|y-x\|^{2}
  \quad\mbox{for all $x,y$ and all $q\in\partial\psi_k(x)$.}
\]
Choose $x=x_{k+1}:=\text{argmin}\,\phi_{k}(x)$. Because
$0\in\partial\psi_k(x_{k+1})$, we have
$$\psi_{k}(y)\geq\psi_{k}(x_{k+1})+\frac{L}{2}\|y-x_{k+1}\|^{2},$$
which, after simplifying, yields the required result.
\end{proof}


\begin{lem}\label{lem:fourbounds} Let $\bar{x}_k$ be the projection 
of $x_k$ onto $\Soln$. Then
\begin{subequations}
\begin{align} 
  \|x_{k}-\bar{x}_k\|^{\phantom2}
  &\leq\tau\|x_{k}-x_{k+1}\|+\frac{\tau}{L}\|e_{k}\|;
  \label{eq:a1a}
\\ \|x_{k}-\bar{x}_k\|^{2} 
  &\leq2\tau^{2}\|x_{k}-x_{k+1}\|^{2}
  +\frac{5}{4}(\tau^{2}/L^{2})\|e_{k}\|^{2};
  \label{eq:a1b}
\\ \|x_{k+1}-\bar{x}_k\|^{\phantom2} 
  & \leq(1+\tau)\|x_{k}-x_{k+1}\|+\frac{\tau}{L}\|e_{k}\|;
  \label{eq:a1c}
\\ \|x_{k+1}-\bar{x}_k\|^{2} & 
  \leq\frac{1}{2}[2+5\tau+3\tau^{2}]\|x_{k}-x_{k+1}\|^{2}
  +\tfrac{1}{2L^{2}}[3\tau^{2}+\tau]\|e_{k}\|^{2}.
  \label{eq:a1d}
\end{align}
\end{subequations}
\end{lem}
\begin{proof}
 For all $k$,
\begin{align*}
\|x_{k}-\bar{x}_k\|
  &\overset{(i)}{\leq}\tau\|x_{k}
  - [x_{k}-\tfrac{1}{L}\nabla f(x_{k})]_{+}\|
\\&\leq
    \tau \|x_{k}-x_{k+1}\|
  + \tau \|x_{k+1}
  - [x_{k}-\tfrac{1}{L}\nabla f(x_{k})]_{+}\|
\\&=\tau \|x_{k}-x_{k+1}\|
  + \tau \|[x_{k}-\tfrac{1}{L}(\nabla f(x)+e_{k})]_{+}
          -[x_{k}-\tfrac{1}{L}\nabla f(x_{k})]_{+}\|
\\& \overset{(ii)}{\leq}\tau\|x_{k}-x_{k+1}\|+\tfrac{\tau}{L}\|e_{k}\|,
\end{align*}
where $(i)$ follows from Assumption~\eqref{a:natural-res} and $(ii)$
follows from the nonexpansiveness of the proximal operator.

\paragraph{\bf Part \eqref{eq:a1b}}Square both sides of~\eqref{eq:a1a}
and then apply the inequality
\begin{equation}\label{eq:crossterm}
ab\leq\frac{a^{2}}{2\alpha}+\frac{\alpha b^{2}}{2},
\quad \forall\alpha>0,
\end{equation}
to bound the cross terms:
\begin{align*}
  \|x_{k}-\bar{x}_k\|^{2}
  & \leq\tau^{2}\|x_{k}-x_{k+1}\|^{2}
  +(\tau/L)^{2}\|e_{k}\|^{2}+(\tau^{2}/L)\|x_{k}-x_{k+1}\|\|e_{k}\|
\\& \leq\big(\tau^{2}
  +\tfrac{\tau^{2}\alpha}{2L}\big)\|x_{k}-x_{k+1}\|^{2}
  +\big(\tfrac{\tau^{2}}{L^{2}}
  +\tfrac{\tau^{2}}{2L\alpha}\big)\|e_{k}\|^{2}\qquad(\forall\alpha>0)
\\& \leq2\tau^{2}\|x_{k}-x_{k+1}\|^{2}
  +\tfrac{5}{4}(\tau^{2}/L^{2})\|e_{k}\|^{2}.
\end{align*}

\paragraph{\bf Part \eqref{eq:a1c}} Use the triangle inequality
and~\eqref{eq:a1a}:
\[
  \|x_{k+1}-\bar{x}_k\|
  \leq\|x_{k+1}-x_{k}\|+\|x_{k}-\bar{x}_k\|
  \leq(1+\tau)\|x_{k}-x_{k+1}\|+(\tau/L)\|e_{k}\|.
\]

\paragraph{\bf Part \eqref{eq:a1d}} Square both sides above, and use the
same technique used in Part~\eqref{eq:a1b} to bound the cross-terms:
\[
  \|x_{k+1}-\bar{x}_k\|^{2}
  \leq\tfrac{1}{2}(2+5\tau+3\tau^{2})\|x_{k}
  -x_{k+1}\|^{2}+\tfrac{1}{2L^{2}}(3\tau^{2}+\tau)\|e_{k}\|^{2}.
\]
\end{proof}

\begin{lem}[Sufficient decrease] 
\label{lem:sufficient_decrease} For all $k$, 
\[
\pi_{k+1}\leq\left(1-\frac{1}{1+40\tau^{2}}\right)\pi_{k}
   +\frac{1}{L}\cdot\frac{40\tau^{2}}{1+40\tau^{2}}\|e_{k}\|^{2}.
\]
\end{lem}

\begin{proof} First, specialize Lemma~\ref{le:3-points} with $y=x_{k}$:
\begin{equation} \label{eq:3pta}
g(x_{k+1}) \leq g(x_{k})-\left\langle \nabla f(x_{k})
  +e_{k},x_{k+1}-x_{k}\right\rangle -L\|x_{k+1}-x_{k}\|^{2}.
\end{equation}
Then,
\begin{align*}
h(x_{k+1}) & \overset{(i)}{\leq}f(x_{k})+\left\langle \nabla f(x_{k}),x_{k+1}-x_{k}\right\rangle +\frac{L}{2}\|x_{k+1}-x_{k}\|^{2}+g(x_{k+1})\\
 & \overset{(ii)}{\leq}f(x_{k})+\left\langle \nabla f(x_{k}),x_{k+1}-x_{k}\right\rangle +\frac{L}{2}\|x_{k+1}-x_{k}\|^{2}+g(x_{k})\\
 &\qquad-\left\langle \nabla f(x_{k})+e_{k},x_{k+1}-x_{k}\right\rangle -L\|x_{k+1}-x_{k}\|^{2}\\
 &= h(x_{k})-\left\langle e_{k},x_{k+1}-x_{k}\right\rangle -\frac{L}{2}\|x_{k}-x_{k+1}\|^{2}\\
 &\leq h(x_{k})+\tfrac{1}{2\alpha}\|e_{k}\|^{2}+\Big(\frac{\alpha}{2}-\frac{L}{2}\Big)\|x_{k}-x_{k+1}\|^{2},
\end{align*}
where $(i)$ uses Assumption~\eqref{a:lipschitz} and $(ii)$ uses
the~\eqref{eq:3pta}. Choose $\alpha=L/2$ and rearrange terms to obtain
the required result.
\end{proof}

We now proceed with the proof of Lemma~\ref{lem:obj-val-bnd}.  Let
$\bar{x}_k$ be the projection of $x_k$ onto the solution set
$\Soln$. By the mean value theorem,
\begin{equation} \label{eq:eq_mv}
  f(x_{k+1})-f(\bar{x}_k)
  = \left\langle \nabla f(\xi),x_{k+1}-\bar{x}_k\right\rangle.
\end{equation}
From Lemma~\ref{le:3-points}, we have
\begin{align} \label{eq:eq_3pt1}
g(x_{k+1})-g(\bar{x}_k) 
& \leq-\left\langle \nabla f(x_{k})
  +e_{k},x_{k+1}-\bar{x}_k\right\rangle \nonumber 
  -\frac{L}{2}\|x_{k+1}-x_{k}\|^{2}
  -\frac{L}{2}\|\bar{x}_k-x_{k+1}\|^{2}
  +\frac{L}{2}\|\bar{x}_k-x_{k}\|^{2}\nonumber \\
& \leq-\left\langle \nabla f(x_{k})
  +e_{k},x_{k+1}-\bar{x}_k\right\rangle 
  +\frac{L}{2}\|\bar{x}_k-x_{k}\|^{2}.
\end{align}
Also note that
\begin{align*}
\left\langle \nabla f(\xi)-\nabla f(x_{k}),x_{k+1}-\bar{x}_k\right\rangle 
 &\leq\|\nabla f(\xi)-\nabla f(x_{k})\|\|x_{k+1}-\bar{x}_k\|
\\&\overset{(i)}{\leq}L\|\xi-x_{k}\|\|x_{k+1}-\bar{x}_k\|
\\&\leq L[\|x_{k+1}-x_{k}\|+\|x_{k}-\bar{x}_k\|]\cdot\|x_{k+1}-\bar{x}_k\|
\\&\leq[L(1+\tau)\|x_{k}-x_{k+1}\|+\tau\|e_{k}\|]
\cdot[(1+\tau)\|x_{k}-x_{k+1}\|+\tfrac{\tau}{L}\|e_{k}\|]
\\&= L(1+\tau)^{2}\|x_{k}-x_{k+1}\|^{2}
 +2[\tau(1+\tau)]\|x_{k}-x_{k+1}\|\|e_{k}\|+\tau^{2}/L\|e_{k}\|^{2}
\\&\leq[L(1+\tau)^{2}+\tfrac{1}{\alpha}\tau(1+\tau)]\|x_{k}-x_{k+1}\|^{2}
+[\tau^{2}/L+\alpha\tau(1+\tau)]\|e_{k}\|^{2}
\\&\leq L(1+3\tau+2\tau^{2})\|x_{k}-x_{k+1}\|^{2}+\tfrac{1}{L}(2\tau^{2}+\tau)\|e_{k}\|^{2},
\end{align*}
where $(i)$ follows from~\eqref{a:lipschitz}. In the steps which
follow, we apply the relevant inequalities in
Lemma~\ref{lem:fourbounds}, group terms, bound every cross term
using~\eqref{eq:crossterm}, and repeat the process until we reach the
final result:
\begin{align*}
h(x_{k+1})-h(\bar{x}_k)
&\overset{(i)}\leq\left\langle \nabla f(\xi)-\nabla f(x_{k}),x_{k+1}-\bar{x}_k\right\rangle -\left\langle e_{k},x_{k+1}-\bar{x}_k\right\rangle +\frac{L}{2}\|\bar{x}_k-x_{k}\|^{2}\\
 & \leq L(1+3\tau+2\tau^{2})\|x_{k}-x_{k+1}\|^{2}+\tfrac{1}{L}(2\tau^{2}+\tau)\|e_{k}\|^{2}\\
 & \qquad+\frac{\alpha}{2}\|e_{k}\|^{2}+\tfrac{1}{2\alpha}\|x_{k+1}-\bar{x}_k\|^{2}+\frac{L}{2}\|\bar{x}_k-x_{k}\|^{2} \qquad \forall \alpha >0\\
 & \leq L(1+3\tau+2\tau^{2})\|x_{k}-x_{k+1}\|^{2}+\tfrac{1}{L}(2\tau^{2}+\tau)\|e_{k}\|^{2}\\
 & \qquad+\frac{\alpha}{2}\|e_{k}\|^{2}+\tfrac{1}{4\alpha}[2+5\tau+3\tau^{2}]\|x_{k}-x_{k+1}\|^{2}\\
 & \qquad +\tfrac{1}{4L^{2}\alpha}[3\tau^{2}+\tau]\|e_{k}\|^{2}+L\tau^{2}\|x_{k}-x_{k+1}\|^{2}+\tfrac{5L}{8}(\tau^{2}/L^{2})\|e_{k}\|^{2}\\
 & \leq\left(L(1+3\tau+2\tau^{2})+\tfrac{1}{4\alpha}[2+5\tau+3\tau^{2}]+L\tau^{2}\right)\|x_{k}-x_{k+1}\|^{2}+\\
 & \qquad\left(\tfrac{1}{L}(2\tau^{2}+\tau)+\frac{\alpha}{2}+\tfrac{1}{4L^{2}\alpha}[3\tau^{2}+\tau]+\tfrac{5L}{8}(\tau^{2}/L^{2})\right)\|e_{k}\|^{2}\\
 & \overset{(ii)}{\leq}10L\tau^{2}\|x_{k}-x_{k+1}\|^{2}+\tfrac{1}{L}10\tau^{2}\|e_{k}\|^{2}\\
 & \overset{(iii)}{\leq}40\tau^{2}[h(x_{k})-h(x_{k+1})]+(4/L^2+\tfrac{1}{L}10\tau^{2})\|e_{k}\|^{2}\\
  &\leq40\tau^{2}[h(x_{k})-h(x_{k+1})]+\tfrac{1}{L}40\tau^{2}\|e_{k}\|^{2}.
\end{align*}
In the steps above, $(i)$ follows by add inequalities~\eqref{eq:eq_mv}
and~\eqref{eq:eq_3pt1}. Also, we make use of
Lemma~\ref{lem:fourbounds} to bound all stray terms in terms of
$\|x_k-x_{k+1}\|^2$ and $\|e_k\|^2$, and Equation~\eqref{eq:crossterm}
to bound the cross-terms. In $(ii)$ we make use of the assumption that
$\tau\geq1$ and set $\alpha=1/L$. Finally, in $(iii)$ we make use of
Lemma~\ref{lem:sufficient_decrease} to transition from a bound on the
distance between successive iterates $x_k$ to differences in
successive values $h(x_k)$.  Rearranging terms, we get
\[
(1+40\tau^{2})h(x_{k+1})-(1+40\tau^{2})h(\bar{x}_k)\leq40\tau^{2}(h(x_{k})-h(\bar{x}_k))+\frac{1}{L}40\tau^{2}\|e_{k}\|^{2},
\]
which is true if and only if the desired result holds:
\[
\pi_{k+1}\leq\left(1-\frac{1}{1+40\tau^{2}}\right)\pi_{k}+\frac{1}{L}\cdot\frac{40\tau^{2}}{1+40\tau^{2}}\|e_{k}\|^{2}.
\]


\begin{example}[Gradient descent with independent Gaussian noise, part
  I] \label{ex:expected-gaussian-noise} 

  Let $e_k\sim N(0,\sigma^2
  I)$. Because $\|e_k\|^2$ is a sum of $n$ independent Gaussians, it
  follows a chi-squared distribution with mean
  $\mathbf{E}\|e_k\|^2=n\sigma^2$. Therefore,
  \begin{equation}\label{eq:gaussian-noise}
    \mathbf{E}\pi_k-\rho^k\pi_0
    \le \frac{1}{\vartheta} \sum_{i=0}^{k-1}\rho^{k-1-i}\mathbf{E}\|e_{i}\|^2
    = \frac{n\sigma^2}{\vartheta} \sum_{i=0}^{k-1}\rho^{k-1-i}.
  \end{equation}
  % Because we wish to investigate the expected value as $N\to\infty$,
  % define
  % \[
  % \pi_\infty = \liminf_{N\to\infty}\pi_k.
  % \]
  Take the limit inferior of both sides of~\eqref{eq:gaussian-noise}, and note that
  $\lim_{k\to\infty}\sum_{i=0}^{k-1}\rho^{k-1-i}=1/(1-\rho) $.  Use
  the values of the constants in Lemma~\ref{lem:obj-val-bnd} to obtain
  the bound
  \[
  \mathbf{E}\liminf_{k\to\infty}\pi_k
  \le\liminf_{k\to\infty}\mathbf{E}\pi_k
  \le \frac{20 \tau^2}{L}\,n\sigma^2,
  \]
  where the first inequality follows from the application of Fatou's
  Lemma~\cite[Ch.~4]{RoydenF:2010}.  Hence, even though
  $\lim_{k\to\infty}\pi_k$ may not exist, we can still provide a lower
  bound on the distance to optimality that is proportional to the
  variance of the error term.
\end{example}

\section{Bounds as functions of Errors}

\subsection{Deterministic Bounds}


\subsection{Bounds in expectation}

Since $\pi_k$ and $e_k$ are both random variables, we can take
expectations on both sides to obtain

\begin{equation*}
  \mathbf{E}\,\pi_k  \le \rho^k \pi_0 
  + \frac{1}{\vartheta}
  \sum_{i=0}^{k-1}\rho^{k-1-i}\,\mathbf{E}\,\|e_i\|^2,
\end{equation*}

\subsection{Probabilistic bounds for gradient descent with random error}
\label{sec:gradient-with-error}

An immediate consequence of Lemma~\ref{lem:obj-val-bnd} is a tail
bound via Markov's inequality:
\[
\Pr(\pi_k-\rho^k\pi_{0} \ge \epsilon) \leq
\Pr \Big( \frac{1}{\vartheta} 
\sum_{i=0}^{k-1}\rho^{k-1-i}\|e_{i}\|^2 \ge
\epsilon \Big)
\le  \frac{1}{\vartheta\epsilon}
\sum_{i=0}^{k-1}\rho^{k-1-i}\mathbf{E}\|e_{i}\|^2.
x\]
This inequality is too weak, however, to say anything meaningful about
the confidence in our solution after a finite number of iterations. We
are instead interested in Chernoff-type bounds that are exponentially
decreasing in $\epsilon$, and in the parameters that control the size
of the error.

The first bound (section~\ref{sec:gener-error-sequ}) that we develop
makes no assumption on the relation of the gradient errors between
iterations, i.e., the error sequence may or may not be history
dependent, and we thus refer to this as a generic error sequence. The
second bound (section~\ref{ubes}) makes the stronger assumption about
the relationship of the errors between iterations.

\subsection{Generic error sequence}\label{sec:gener-error-sequ}

Our first exponential tail bounds are defined in terms of the
moment-generating function
\begin{equation*}
\gamma_k(\theta) := \mathbf{E}\exp(\theta\|e_k\|^2)
\end{equation*}
of the error norms $\|e_k\|^2$. We make the convention that
$\gamma_k(\theta)=+\infty$ for $\theta \notin \text{dom}\gamma_k$.

\begin{thm}[Tail bound for generic errors]
  \label{th: generic tail bound} 
  For algorithm~\eqref{eq:prox-gradient},
  \begin{subequations}
  \begin{equation}
    \label{eq:6}
    \Pr(\pi_k - \rho^k\pi_0 \ge \epsilon)
    \le
    \inf_{\theta>0}
    \left\{
      \frac{\exp(-\theta \vartheta \epsilon/R_k)}{R_k}
      \sum_{i=0}^{k-1}\rho^{k-1-i}\gamma_i(\theta)
    \right\}.
  \end{equation}
  If $\gamma_k\equiv\gamma$ for all $k$ (i.e., the error norms
  $\|e_{k}\|^2$ are identically distributed), then the bound simplifies
  to
  \begin{equation}
    \label{eq:6-simple}
    \Pr(\pi_k - \rho^k\pi_0 \ge \epsilon)
    \le
    \inf_{\theta>0}
    \left\{
      \exp(-\theta \vartheta \epsilon/ R_k) 
      \gamma(\theta)
    \right\}.
  \end{equation}
  \end{subequations}
\end{thm}
\begin{proof}
  By the definition of $R_{k}$, $\left(\sum_{i=0}^{k-1}
    \rho^{k-1-i}\right)/{R_k}=1$. Thus, for $\theta>0$,
  \begin{align*}
    \mathbf{E}
        \exp\left(
              \theta\sum_{i=0}^{k-1}\rho^{k-1-i}\|e_i\|^2
            \right)
     &=
    \mathbf{E}
        \exp\left(
              \sum_{i=0}^{k-1}
              \frac{\rho^{k-1-i}}{R_k}\theta R_k\|e_i\|^2
            \right)
\\  &\overset{(i)}\le
    \mathbf{E}
        \sum_{i=0}^{k-1}\frac{\rho^{k-1-i}}{R_k}
        \exp(\theta R_k\|e_i\|^2)
 \overset{(ii)}=
    \frac{1}{R_k}\sum_{i=0}^{k-1}\rho^{k-1-i}\gamma_i(\theta R_k),
  \end{align*}
  where $(i)$ follows from the convexity of $\exp(\cdot)$, and $(ii)$
  follows from the linearity of the expectation operator and the
  definition of $\gamma_i$. Together with Markov's inequality, the above
  implies that for all $\theta>0$,
  \begin{align}
    \Pr\left(\sum_{i=0}^{k-1}\rho^{k-1-i}\|e_{i}\|^2\ge\epsilon \right)
    &=
    \Pr\left(
      \exp
      \left[
        \theta \sum_{i=0}^{k-1}\rho^{k-1-i}\|e_i\|^2
      \right]
      \ge
      \exp(\theta\epsilon)
    \right) \nonumber
\\ &\le
   \exp(-\theta\epsilon)
   \mathbf{E} \exp\left(
     \theta \sum_{i=0}^{k-1}\rho^{k-1-i}\|e_i\|^2
   \right) \nonumber
\\ &\le
   \frac{\exp(-\theta\epsilon)}{R_k}
   \sum_{i=0}^{k-1}\rho^{k-1-i}\gamma_i(\theta R_k).
   \label{eq:markov-application-sum}
  \end{align}
  This inequality, together with Lemma~\ref{lem:obj-val-bnd}, implies
  that for all $\theta>0$,
  \begin{align*}
    \Pr\left(\pi_k-\rho^k\pi_0\ge\epsilon\right)
    &\le
    \Pr
    \left(
      \frac1{\vartheta}\sum_{i=0}^{k-1}\rho^{k-1-i}\|e_i\|^2
      \ge \vartheta\epsilon
    \right) 
    \le
    \frac{\exp(-\theta \vartheta\epsilon)}{R_k}
    \sum_{i=0}^{k-1}\rho^{k-1-i}\gamma_i(\theta R_k),
  \end{align*}
  where we use the elementary fact that
  $\Pr(X\ge\epsilon)\le\Pr(Y\ge\epsilon)$ if $X\le Y$ almost
  surely. Redefine $\theta$ as $\theta R_k$, and take the infimum
  of the right-hand side over $\theta>0$, which gives the required
  inequality~\eqref{eq:6}. The simplified bound~\eqref{eq:6-simple}
  follows directly from the definition of $R_k$.
\end{proof}

When the errors are identically distributed, there is an intriguing
connection between the tail bounds described in Theorem~\ref{th:
  generic tail bound} and the convex conjugate of the
cumulant-generating function of that distribution, i.e., $(\log\circ
\,\gamma)^{*}$.

\begin{thm}[Tail bound for identically-distributed errors]
  \label{co:simple-log}
  Suppose that the error norms $\|e_k\|^2$ are identically
  distributed. Then for algorithm~\eqref{eq:prox-gradient},
  \begin{equation*}
    \log\Pr(\pi_k - \rho^k\pi_0 \ge \epsilon)
    \le
    -\left[\log\gamma(\cdot)\right]^*(\vartheta\epsilon/R_k).
  \end{equation*}
\end{thm}
\begin{proof}
  Take the log of both sides of~\eqref{eq:6-simple} to get
  \begin{align*}
    \log\Pr(\pi_k - \rho^k\pi_0 \ge \epsilon)
     &\le
     \log\inf_{\theta>0}
     \left\{
       \exp(-\theta \vartheta\epsilon/R_k)\, \gamma(\theta)
     \right\}
   =-\sup_{\theta>0}
   \left\{
     (\vartheta\epsilon/R_k)\theta - \log\gamma(\theta)
   \right\},
  \end{align*}
  which we recognize as the negative of the conjugate of
  $\log\circ\,\gamma$ evaluated at $\vartheta\epsilon/R_k$.
\end{proof}


Note that these bounds are invariant with regard to scaling, in
the sense that if the objective function $f$ is scaled by some
$\alpha>0$, then the bounds hold for $\alpha\epsilon$.

The following example illustrates an application of this tail bound to
the case in which the errors follow a simple distribution with a known
moment-generating function.

\begin{example}[Gradient descent with independent Gaussian noise,
  part~II]
  \label{ex:expected-gaussian-noise-2}
  As in Example~\ref{ex:expected-gaussian-noise}, let
  $e_k\sim N(0,\sigma^2 I)$. Then $e_k$ is a scaled chi-squared
  distribution with moment-generating function
  \[
  \gamma_k(\theta) = (1-2\sigma^2 \theta)^{-n/2},
  \quad
  \theta \in \left[0,\frac1{2\sigma^2}\right).
  \]
  Note that
  \[
  [\log\gamma(\cdot)]^*(\mu)
  = \frac{\mu-n\sigma^{2}}{2\sigma^{2}}+\frac{n}{2}\log(n\sigma^{2}/\mu)
  \text{for}
  \mu > n\sigma^{2}.
  \]
  We can then apply Corollary~\ref{co:simple-log} to this case to
  deduce the bound
  \[
  \Pr(\pi_k-\rho^k\pi_0\ge \epsilon)
  \le
  \left(\frac{\exp(1)}{n}\cdot\frac{\vartheta\epsilon}{\sigma^{2} R_k}\right)^{n/2}\!\!
  \exp\left(-\frac{\vartheta\epsilon}{2 \sigma^{2} R_k}\right)
  \text{for}
  \epsilon>\frac{n\sigma^{2} R_k}{\vartheta}.
  \]
  The bound can be further simplified by introducing an additional
  perturbation $\delta>0$ that increases the base of the exponent:
  \begin{equation}\label{eq:13}
  \Pr(\pi_k-\rho^k\pi_0\ge \epsilon) =
  \mathcal{O}\left[\exp\left(-\delta\frac{\vartheta\epsilon}{2 \sigma^{2}
        R_k}\right)\right] \text{for all} \mbox{$\delta\in[0,1)$},
  \end{equation}
  which highlights the exponential decrease of the bound in terms of
  $\epsilon$.

  
\end{example}

\subsection{Unconditionally bounded error sequence} \label{ubes}

In contrast to the previous section, we now assume that there exists a
deterministic bound on the conditional expectation
$\mathbf{E}\left[\exp(\theta\|e_k\|^2)\mid\mathcal{F}_{k-1}\right]$. We say that this
bound holds unconditionally because it holds irrespective of the
history of the error sequence.

\begin{lem} \label{as:unconditional errors} Assume that
  $\mathbf{E}\left[\exp(\theta\|e_k\|^2)\mid\mathcal{F}_{k-1}\right]$ is finite over
  $[0,\sigma)$, for some $\sigma>0$. Therefore there exists, for each
  $k$, a deterministic function
  $\gamma_k:\Real_{+}\to\Real_{+}\cup\{\infty\}$ such that
  \[
  \gamma_k(0)=1
  \qquad 
  \text{\normalfont and}
  \qquad 
  \mathbf{E}\left[\exp(\theta\|e_k\|^2)\mid\mathcal{F}_{k-1}\right]\le\gamma_k(\theta).
  \]
  (Thus, the bound is tight at $\theta=0$.)
\end{lem}

The existence of such a function in fact implies a bound on the
moment-generating function of $\|e_k\|^2$. In particular,
\begin{equation} \label{eq:mgfbound}
\gamma_k(\theta) := 
\mathbf{E} \exp(\theta\|e_k\|^2) =
\mathbf{E}\left[
      \mathbf{E}\left[
        \exp(\theta\|e_k\|^2)\mid\mathcal{F}_{k-1}
      \right]
    \right]
\le\mathbf{E} \gamma_k(\theta) =
\gamma_k(\theta).
\end{equation}
The converse, however, is not necessarily true. To see this, consider
the case in which the errors $e_1,\ldots,e_{k-1}$ are independent
Bernoulli-distributed random variables, and $e_k$ is a deterministic
function of all the previous errors, e.g., $\Pr(e_i=0) = \Pr(e_i=1) =
1/2$ for $i=1,\ldots,k-1$, and the error on the last iteration is
completely determined by the previous errors:
\[
e_k =
\begin{cases}
  1 & \mbox{if $e_1 = e_2 = \cdots = e_{k-1}$},
\\0 & \mbox{otherwise}.
\end{cases}
\]
Therefore, $\Pr(e_k=1) = (1/2)^{k-1}$ and $\Pr(e_k=0) =
1-(1/2)^{k-1}$, and the moment-generating function of $e_k$ is
$
\gamma_k(\theta) = 1-{2^{1-k}}(1+\exp\theta).
$
Then,
\[
\mathbf{E}[\exp(\theta e_k^2)\mid e_1,\ldots,e_{k-1}] =
\begin{cases}
  \exp\theta   & \mbox{if $e_1 = e_2 = \cdots = e_{k-1}$},
\\1            & \mbox{otherwise,}
\end{cases}
\]
whose tightest deterministic upper bound is
$\gamma_k(\theta)=\exp\theta$. However,
$\gamma_k(\theta)\ge\gamma_k(\theta)$ for all $\theta\ge0$.
The following result is analogous to Theorem~\ref{th: generic tail bound}.

\begin{thm}[Tail bounds for unconditionally bounded errors]
  \label{th:unconditional error bound}
  Suppose that Assumption~\ref{as:unconditional errors} holds. Then
  for algorithm~\eqref{eq:prox-gradient},
  \begin{equation*}
    \Pr(\pi_k-\rho^k\pi_0\ge\epsilon)
    \le
    \inf_{\theta > 0}
    \left\{
      \exp(-\theta \vartheta\epsilon)
      \prod_{i=0}^{k-1}\gamma_i(\theta\rho^{k-i-1})
    \right\}.
  \end{equation*}
\end{thm}
\begin{proof}
  The proof follows the same outline as many martingale-type
  inequalities \cite{azuma1967weighted,chung2006concentration}. We
  obtain the following relationships:
  \begin{align*}
    \mathbf{E} \exp\left[ \theta\sum_{i=0}^{k-1}\rho^{k-1-i}\|e_i\|^2 \right]
    &\overset{(i)}{=}
    \mathbf{E}\left[
        \mathbf{E}\left[
            \exp\left.
                \left[
                 \theta\sum_{i=0}^{k-1}\rho^{k-1-i}\|e_i\|^2
                \right]
                \right|\mathcal{F}_k-2
           \right]
       \right]
\\  &=
    \mathbf{E}\left[
        \mathbf{E}\left[
            \exp\left.
                \left[
                 \theta\rho^0 \|e_{k-1}\|^2+\theta\sum_{i=0}^{k-2}\rho^{k-1-i}\|e_i\|^2
                \right]
                \right|\mathcal{F}_{k-2}
           \right]
       \right]  
\\  &\overset{(ii)}{=}
    \mathbf{E}\!\left[
            \exp\left[
                 \theta\sum_{i=0}^{k-2}\rho^{k-1-i}\|e_i\|^2
                 \right]
            \mathbf{E}\left[
                  \left.
                  \exp\left(
                        \theta\|e_{k-1}\|^2
                       \right)
                  \right| \mathcal{F}_k-2
               \right]
       \right]
\\    &\overset{(iii)}{\leq}
      \mathbf{E} \left[
        \exp \left[
          \theta \sum_{i=0}^{k-2}\rho^{k-i-1}\|e_i\|^2
          \right]
        \right]
      \gamma_{k-1}(\theta)
\\    &\overset{(iv)}{\leq}
      \prod_{i=0}^{k-1}\gamma_i(\theta \rho^{k-i-1}),
\end{align*}
where $(i)$ follows from the law of total expectations, i.e.,
$\mathbf{E}_Y[\mathbf{E}[X|Y]]=\mathbf{E}[X]$; $(ii)$ follows from the observation that the
random variable $\exp(\theta\sum_{i=0}^{k-2}\rho^{k-1-i}\|e_i\|^2)$ is
a deterministic function of $e_0,\ldots,e_k-2$, and hence is
measurable with respect to $\mathcal{F}_{k-1}$ and can be factored out of the
expectation; $(iii)$ uses Assumption~\ref{as:unconditional errors};
and to obtain $(iv)$ we simply repeat the process recursively.

Thus, we now have a bound on the moment-generating function of the
discounted sum of errors
$\theta\sum_{i=0}^{k-1}\rho^{k-1-i}\|e_i\|^2$, and we can continue by
using the same approach used to
derive~\eqref{eq:markov-application-sum}. The remainder of the proof
follows that of Theorem~\ref{th: generic tail bound}, except that the
sums over $i=0,\ldots,k$ are replaced by products over that same
range.
\end{proof}

In an application where both $\gamma_k$ and $\gamma_k$ are available, it
is not true in general that either of the bounds obtained in
Theorems~\ref{th: generic tail bound} and~\ref{th:unconditional error
  bound} are tighter than the other. When only a bound $\gamma_k$ that
satisfies Assumption~\ref{as:unconditional errors} is available,
however, (which is the case in the sampling application we shall describe)
% TODO section~\ref{sa}) 
we could leverage~\eqref{eq:mgfbound} and apply
Theorem~\ref{th: generic tail bound} to obtain a valid bound in terms
of $\gamma_k$ by simply substituting it for $\gamma_k$. However, as shown
below, in this case it is better to apply
Theorem~\ref{th:unconditional error bound} because it yields a
uniformly better bound:
\begin{equation}\label{eq:unconditional-bound}
  \Pr\left(\pi_k-\rho^k\pi_k\geq\epsilon\right)
  \leq
  \inf_{\theta>0}
  \left\{
    \exp
    \left(
      -\theta \vartheta
      \epsilon+\sum_{i=0}^{k-1}\log\gamma_{i}\big(\theta\rho^{k-1-i}\big)
    \right)
  \right\},
\end{equation}
while Theorem~\ref{th: generic tail bound} (with $\gamma_k$ replaced by
$\gamma_k$) gives us
\begin{equation} \label{eq:10}
  \Pr\left(\pi_k-\rho^k\pi_{0}\geq\epsilon\right)
  \leq
  \inf_{\theta>0}
  \left\{
    \exp\left(-\theta \vartheta\epsilon
      +\log\left[
        \frac{1}{R_k}\sum_{i=0}^{k-1}\rho^{k-1-i}\gamma_{i}(\theta R_k)
      \right]
    \right)
  \right\},
\end{equation}
where we rescale $\theta$ by $R_k$.  A direct comparison of the two
bounds show that they only differ by one term:
\[
  \log\left[
    \frac{1}{R_{k}}\sum_{i=0}^{k-1}\rho^{k-i-1}\gamma_{i}(\theta R_k)
  \right]\quad 
  \text{\normalfont vs.}\quad 
  \sum_{i=0}^{k-1}\log\gamma_{i}(\theta\rho^{k-1-i}).
\]
Because $R_k=\sum_{i=0}^{k}\rho^{k-i-1}$, the term in the
$\log$ on the left is a convex combination of the functions
$\gamma_{i}$. Therefore,
\begin{align*}
  \log\left[
    \frac{1}{R_{k}}\sum_{i=0}^{k-1}\rho^{k-i-1}\gamma_{i}(\theta R_k)
  \right]
    & \overset{(i)}{\ge}
  \sum_{i=0}^{k-1}\frac{\rho^{k-1-i}}{R_k}\log\gamma_{i}(\theta R_k)
  \\& \overset{(ii)}{\ge}
  \sum_{i=0}^{k-1}\log\gamma_{i}(\theta R_k\,\rho^{k-1-i}/R_k)
  \\& = 
  \sum_{i=0}^{k-1}\log\gamma_{i}(\theta\rho^{k-1-i}),
\end{align*}
where $(i)$ is an application of Jensen's inequality and the concavity
of $\log$, and $(ii)$ follows from the convexity of the cumulant
generating function. It is then evident that~\eqref{eq:unconditional-bound}
implies~\eqref{eq:10}.

As with Corollary~\ref{co:simple-log}, by taking logs of both sides
above, a connection can be made between our bound and the infimal
convolution when $\bar{\gamma}$ is log-concave:
\begin{align*}
\log \Pr(\pi_k-\rho^{k}\pi_{0}\ge\epsilon) & \le 
  \left[\bigotimes_{i=0}^{k-1}\, [\log\gamma_{i}(\,\cdot\,\rho^{k-i-1})]^{*}
  \right](\vartheta\epsilon/R_{k}),
\end{align*}
where $\otimes$ denotes the infimal convolution operator.

\begin{example}[Gradient descent with independent Gaussian noise, part
  III]\label{ex:ex-gaussian-noise-3}
  As in Example~\ref{ex:expected-gaussian-noise-2}, let $e_k\sim
  N(0,\sigma^2 I)$. Because the errors $e_k$ are independent,
  $\mathbf{E}\big[\exp(\theta\|e_k\|^2)\,|\,\mathcal{F}_{k-1}\big]=\mathbf{E}\exp(\theta\|e_k\|^2)=\gamma_k(\theta)$,
  which satisfies Assumption~\ref{as:unconditional errors} with
  $\gamma_k(\theta):= \gamma_k(\theta)$. Apply
  Theorem~\ref{th:unconditional error bound} to obtain the bound
  \begin{align}\label{eq:12}
    \Pr \, ( 
    \pi_k-\rho^k\pi_{0}
      \geq\epsilon
      )
    &\leq
    \inf_{\theta>0}
    \left\{
      \exp(-\theta \vartheta \epsilon) \cdot 
      \prod_{i=0}^{k-1}(1-2\sigma^2\theta\rho^{k-1-i})^{-n/2}\right\}.
  \end{align}
Apply Lemma~\ref{fact:Q-Pochhammer_Lower_Bound} to obtain
\[
  \Pr\,
  (
    \pi_k-\rho^k\pi_{0}
      \geq\epsilon
  )
  \overset{}{\leq}
  \left(
    \frac{\exp(1)}{n\alpha}\cdot\frac{ \vartheta \epsilon }{\sigma^2}
  \right)
  ^{\tfrac{n\alpha}{2}} \!\!
  \exp
  \left(
    -\frac{\vartheta\epsilon}{\sigma^2}
  \right)
  \text{for} 
  \epsilon > \frac{n\alpha\sigma^2}{\vartheta},
\]
where $\alpha=1-(\log\rho)^{-1}.$ We simplify the bound to obtain
  \begin{equation}\label{eq:14}
  \Pr\,(\pi_k-\rho^k\pi_0\ge \epsilon) =
  \mathcal{O}\left[\exp\left(- \,\delta\cdot \frac{\vartheta\epsilon}{\sigma^{2}}\right)\right] \text{for all} \mbox{$\delta\in(0,1)$};
  \end{equation}
cf.~\eqref{eq:13}.

As an aside, we note that we can easily accommodate correlated noise,
i.e., $e_k\sim N(0,\Sigma^{2})$ where $\Sigma$ is an $n\times n$ positive
definite matrix. The error $\|e_k\|^2$ then has the
distribution of a sum of chi-squared random variables that are
weighted according to the eigenvalues $\sigma_j$ of
$\Sigma$~\cite{imhof:1961}:
  \[
  \|e_k\|^2 \sim \sum_{j=1}^{n}\sigma_{j}^{2}\chi_1^2,
  \]
  and so the above tail bounds hold with $\sigma=\sigma_{\text max}$.
\end{example}{

  The bounds obtained in Examples~\ref{ex:expected-gaussian-noise-2}
  and~\ref{ex:ex-gaussian-noise-3} illustrate the relative strengths
  of Theorems~\ref{th: generic tail bound} and~\ref{th:unconditional
    error bound}. Comparing~\eqref{eq:13} and~\eqref{eq:14}, we see
  that the asymptotic bounds only differ by a factor of
  $1/R_k$. Hence, for large $\epsilon$, the bound in
  Example~\ref{ex:expected-gaussian-noise-2} is uniformly weaker than
  the bound in Example~\ref{ex:ex-gaussian-noise-3}. Note that this
  holds despite the simplification (i.e.,
  Lemma~\ref{fact:Q-Pochhammer_Lower_Bound}) used to
  simply~\eqref{eq:12}.

